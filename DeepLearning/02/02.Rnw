<<setup-child, include = FALSE>>=
library(knitr)
set_parent("../style/preamble.Rnw")
@

<<size = "scriptsize", include=FALSE>>=
source("code/functions.R")
@

\input{../latex-math/basic-math}
\input{../latex-math/basic-ml}
\input{../latex-math/ml-nn}

\lecturechapter{2}{Deep Learning- A Single Neuron}
\lecture{Deep Learning- A Single Neuron}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame} {A Single Neuron}
\begin{itemize}
\item To illustrate the types of functions that neural networks can represent, let us begin with a simple model: logistic regression.
\vspace{5mm}
\item The hypothesis space of logistic regression can be written as 
\begin{small}
$$\Hspace = \left\{f: \R^p \to [0, 1] ~\bigg|~ \fx = \tau\left(\sum_{j = 1}^p w_j x_j + b\right), \wtw \in \R^p, b \in \R \right\},$$
\end{small}
where $\tau(z) = (1 + \exp(-z))^{-1}$ is the logistic sigmoid function.
\vspace{5mm}
\item It is straightforward to represent $\fx$ graphically as a neuron.
\vspace{5mm}
\item Note: $\wtw$ and $b$ together constitute $\thetab$.
\end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe} {A Single Neuron}
\begin{figure}
\centering
\scalebox{.4}{\includegraphics{plots/perceptron_tau.png}}
\end{figure}
\vspace{-2mm}
\footnotesize Perceptron $z$, with \textbf{input features} $x_1, x_2, ... ,x_p$, \textbf{weights} $w_1, w_2,... ,w_p$, \textbf{bias term} $b$ and \textbf{activation function} $\tau$.
\vspace{.2cm}
\normalsize
\begin{itemize}
\item The perceptron is the basic computational unit for neural networks.
\vspace{.2cm}
\item It is a weighted sum of input values, transformed by $\tau$:
\vspace{-1mm}
$$y = \tau(w_1x_1 + ... + w_px_p +  b) = \tau(w^\top x+b)$$
\end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{A Single Neuron}
\textbf{Choices for $\tau$:} a single neuron can represent different functions if we choose suitable activation function for it.
\vspace{.5cm}
\begin{itemize}
\item The identity function gives us the simple \textbf{linear regression}:
$$y = \tau(w^Tx) = w^Tx$$
\vspace{.5cm}
\item The logistic function gives us the \textbf{logistic regression}:
$$y = \tau(w^Tx) = \frac{1}{1 + \exp(-w^Tx)}$$
\end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe} {A Single Neuron}
We consider a logistic regression model for $p = 3$, i.e. $f(\xv) = \tau(w_1\textcolor{red}{x_1} + w_2\textcolor{red}{x_2} + w_3\textcolor{red}{x_3} + b)$.
\begin{itemize}
\item First, features of $\xv$ are represented by nodes in the \enquote{input layer}.
\begin{figure}
\includegraphics[width=6cm]{plots/neurep_one.png}
\end{figure}
\item In general, a $p$-dimensional input vector $\xv$ will be represented by $p$ nodes in the input layer.
\framebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\item Next, weights $\mathbf{w}$ are represented by edges from the input layer.
\begin{figure}
\includegraphics[width=6cm]{plots/neurep_two.png}
\end{figure}
\item The bias term $b$ is implicit. It is not shown visually.
\end{itemize}
\framebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

For an explicit graphical representation, we do a simple trick: 
\begin{itemize}
\item Add a constant feature to the inputs $\tilde{\xv} = (1, x_1, ..., x_p)^\top$
\item and the bias term to the weight vector $\tilde{\bm{w}} = (b, w_1, ..., w_p)$.
\end{itemize}
The graphical representation is then: 
\begin{figure}
\includegraphics[width=7cm]{plots/neurep_bias.png}
\end{figure}
\framebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{itemize}
\item Finally, the computation $\tau(w_1x_1 + w_2x2 + w_3x_3 + b)$ is represented by the neuron in the \enquote{output layer}.
\begin{figure}
\includegraphics[width=6cm]{plots/neurep_three.png}
\end{figure}
\item Because this single neuron represents exactly the same hypothesis space as logistic regression, it can only learn linear decision boundaries.
\framebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\item A nice thing about this graphical representation of functions is that you can picture the input vector being "fed" to the neuron on the left followed by a sequence of computations being performed from left to right. This is called a \textbf{forward pass}.
\end{itemize}
\vspace{1cm}
\begin{figure}
\includegraphics[width=5.5cm]{plots/forward_pass.png}
\end{figure}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame} {A Single Neuron}
Therefore, a neuron performs a 2-step computation:
\begin{enumerate}
\item \textbf{Affine Transformation:} weighted sum of inputs plus bias.
\begin{figure}
\includegraphics[width=5.5cm]{plots/step1.png}
\end{figure}
\item \textbf{Non-linear Activation:} a non-linear transformation applied to the weighted sum.
\begin{figure}
\includegraphics[width=5.5cm]{plots/step2.png}
\end{figure}
\end{enumerate}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe} {A Single Neuron}
\begin{itemize}
\item Even though all neurons compute a weighted sum in the first step, there is considerable flexibility in the type of activation function used in the second step.
\item For example, setting the activation function to logistic sigmoid function allows a neuron to represent logistic regression. The following neuron represents a logistic regression with two outputs.
\end{itemize}
\begin{figure}
\includegraphics[width=5.5cm]{plots/logistic_regression.png}
\end{figure}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{A Single Neuron}
\begin{itemize}
\item The hypothesis space that is formed by single neuron architectures is 
\begin{small}
$$\Hspace  = \left\{f: \R^p \to \R ~\bigg|~ \fx = \tau\left(\sum_{j = 1}^p w_j x_j + b\right), \wtw \in \R^p, b \in \R\right\}.$$ 
\end{small}
\item Both logistic regression and linear regression are subspaces of $\Hspace$ (if $\tau$ is the logistic sigmoid / identity function).  \end{itemize}
\vspace*{-0.48cm}
\begin{figure}
\centering
\scalebox{0.9}{\includegraphics{plots/neuron_regcls.png}}
\vspace*{-0.2cm}
\begin{tiny}
\caption{\textit{Left}: A regression line learned by a single neuron. \textit{Right}: A decision-boundary learned by a single neuron in a binary classification task.}
\end{tiny}
\end{figure}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe} {A Single Neuron: Optimization}
\begin{itemize}
\item To optimize this model, we minimize the empirical risk 
$$\riske = \sumin \Lxyi,$$
where $\Lxy$ is a loss function. It compares the network's predictions $\fx$ to the ground truth $y$. 
\item For regression, we typically use the L2 loss (rarely L1): $$\Lxy = \frac{1}{2}(y - \fx)^2$$
\item For binary classification, we typically apply the cross entropy loss (also known as bernoulli loss): $$\Lxy = y \log \fx + (1 - y) \log(1 - \fx)$$
\framebreak 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\vspace{.5cm}
\item For a single neuron, in both cases, the loss function is convex and the global optimum can be found with an iterative algorithm like gradient descent. 
\vspace{1cm}
\item In fact, a single neuron with logistic sigmoid function trained with the bernoulli loss does not only have the same hypothesis space as a logistic regression and is therefore the same model, but will also yield to the very same result when trained until convergence.
\end{itemize}
\end{vbframe} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\endlecture