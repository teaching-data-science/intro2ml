%This file is a child of preamble.Rnw in the style folder
%if you want to add stuff to the preamble go there to make
%your changes available to all childs

<<setup-child, include = FALSE>>=
library(knitr)
set_parent("../style/preamble.Rnw")
@

<<size = "scriptsize", include=FALSE>>=
source("code/functions.R")
@

\input{../latex-math/basic-math}
\input{../latex-math/basic-ml}
\input{../latex-math/ml-nn}

\lecturechapter{2}{Deep Learning- A Single Neuron}
\lecture{Deep Learning- A Single Neuron}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame} {A Single Neuron}
  \begin{itemize}
    \vspace{5mm}
    \item In order to better understand the types of functions that neural networks can represent, let us begin with a very simple model: logistic regression.
    \item Recall: The hypothesis space of logistic regression can be written as 
    \begin{small}
    $$
    \Hspace = \left\{f: \R^p \to [0, 1] ~\bigg|~ \fx = \tau\left(\sum_{j = 1}^p w_j x_j + b\right), \wtw \in \R^p, b \in \R \right\},
    $$
    \end{small}
    where $\tau(z) = (1 + \exp(-z))^{-1}$ is the logistic sigmoid function.
    \item It is straightforward to represent this function $\fx$ graphically as a neuron.
    \item Note: $\wtw$ and $b$ together constitute $\thetab$.
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe} {A Single Neuron}

\begin{figure}
\centering
\scalebox{0.7}{\includegraphics{plots/perceptron_neu_tau.png}}
\end{figure}
\vspace{-2mm}
\footnotesize Perceptron $z$, with \textbf{input features} $x_1, x_2, ... ,x_p$, \textbf{weights} $w_1, w_2,... ,w_p$, \textbf{bias term} $b$ and \textbf{activation function} $\tau$.

\vspace{.7cm}
\normalsize
\begin{itemize}
\item The perceptron is the basic computational unit for neural networks.
\item It is a weighted sum of input values, transformed by $\tau$:
\vspace{-1mm}
$$y = \tau(w_1x_1 + ... + w_px_p +  b) = \tau(w^\top x+b)$$
\end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{A Single Neuron}

\textbf{Choices for $\tau$:} a single neuron can represent different functions if we choose suitable activation function for it.
\vspace{.5cm}

\begin{itemize}
\item If we choose the identity, the model becomes a simple \textbf{linear regression}:
$$y = \tau(w^Tx) = w^Tx$$
\vspace{.5cm}
\item If we choose the logistic function, it gives us the \textbf{logistic regression}:
$$y = \tau(w^Tx) = \frac{1}{1 + \exp(-w^Tx)}$$
\end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame} {A Single Neuron}
We consider a logistic regression model for $p = 3$, i.e. $f(\xv) = \tau(w_1\textcolor{red}{x_1} + w_2\textcolor{red}{x_2} + w_3\textcolor{red}{x_3} + b)$.

\begin{itemize}
\item First, features of $\xv$ are represented by nodes in the \enquote{input layer}.
\begin{figure}
\centering
\scalebox{0.7}{\includegraphics{plots/neurep_one.png}}
\end{figure}
\item In general, a $p$-dimensional input vector $\xv$ will be represented by $p$ nodes in the input layer.
\end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame} {A Single Neuron}
We consider a logistic regression model for $p = 3$, i.e. $f(\xv) = \tau(w_1\textcolor{red}{x_1} + w_2\textcolor{red}{x_2} + w_3\textcolor{red}{x_3} + b)$.

\begin{itemize}
\item Next, weights $\mathbf{w}$ are represented by edges from the input layer.
\begin{figure}
\centering
\scalebox{0.7}{\includegraphics{plots/neurep_two.png}}
\end{figure}
\item The bias term $b$ is implicit. It is not shown/represented visually.
\end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{A Single Neuron}
\textbf{Note:} For an explicit graphical representation of the bias term, we can do a simple trick: 
\begin{itemize}
  \item we add a constant feature to the inputs $\tilde{\xv} = (1, x_1, ..., x_p)^\top$
  \item and the bias term to the weight vector $\tilde{\bm{w}} = (b, w_1, ..., w_p)$.
\end{itemize}

The graphical representation is then: 

\vspace*{-0.1cm}
\begin{figure}
\scalebox{0.60}{\includegraphics{plots/neurep_bias.png}}
\caption{Weights and bias of the neuron.}
\end{figure}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame} {A Single Neuron}
We consider a logistic regression model for $p = 3$, i.e. $f(\xv) = \tau(w_1\textcolor{red}{x_1} + w_2\textcolor{red}{x_2} + w_3\textcolor{red}{x_3} + b)$.

  \begin{itemize}
    \item Finally, the computation $\tau(w_1x_1 + w_2x2 + w_3x_3 + b)$ is represented by the neuron in the \enquote{output layer}.
    \begin{figure}
    \centering
      \scalebox{0.7}{\includegraphics{plots/neurep_three.png}}
  \end{figure}
  \end{itemize}
  \vspace*{-0.5cm}
    \small{Because this single neuron represents exactly the same hypothesis space as logistic regression, it can only learn linear decision boundaries.}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe} {A Single Neuron}
  \begin{itemize}
    \item Therefore, a neuron is just a graphical representation of a very specific kind of function.
    \vspace{4mm}
    \item Every neuron performs a 2-step computation:
      \begin{itemize}
        \item Step 1: compute the weighted sum of inputs (with bias).
        \item Step 2: apply an \textbf{activation function} to the sum,  which is usally  a non-linear transformation of the input.
      \end{itemize}
    \vspace{4mm}
    \item With a single neuron, the activation function serves to constrain the output to the desired range of values. In case of logistic regression, it squashes the output into $[0, 1]$. 
    \vspace{4mm}
    \item However, we will see that activation functions serve a far more important purpose. They are one of the main reasons that neural networks can represent extremely complicated functions.
  \end{itemize}
\end{vbframe}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe} {A Single Neuron}
  \begin{itemize}
    \item \small{One very nice thing about the graphical representation of the function is that you can picture the input vector being "fed" to the neuron on the left followed by a sequence of computations being performed from left to right. This is called a \enquote{\textbf{forward pass}}}.
  \end{itemize}
  \begin{figure}
    \centering
      \only<1>{\scalebox{0.60}{\includegraphics{plots/neuron_one.png}}
            \caption{Weights (and bias) of the neuron.}}
      \only<2>{\scalebox{0.60}{\includegraphics{plots/neuron_two.png}}      \caption{Feed the input on the left.}}
      \only<3>{\scalebox{0.60}{\includegraphics{plots/neuron_three.png}}\caption{Step 1: Compute the weighted sum.}}
      \only<4>{\scalebox{0.60}{\includegraphics{plots/neuron_six.png}} \caption{Step 2: Apply the activation function.}}
      \only<5>{\scalebox{0.60}{\includegraphics{plots/neuron_seven.png}}}
  \end{figure}
\end{vbframe}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe} {A Single Neuron}
  \begin{itemize}
    \item \small{Even though all neurons compute a weighted sum in the first step, there is considerable flexibility in the type of activation function used in the second step.
    \item For example, setting the activation function to the identity function allows a neuron to represent linear regression.}
  \begin{figure}
    \centering
      \scalebox{0.6}{\includegraphics{plots/neuron_five.png}}
  \end{figure}  
  \end{itemize}
\end{vbframe}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{A Single Neuron}
  
\begin{itemize}
  \item The hypothesis space that is formed by single neuron architectures is 
  \begin{small}
    $$
      \Hspace  = \left\{f: \R^p \to \R ~\bigg|~ \fx = \tau\left(\sum_{j = 1}^p w_j x_j + b\right), \wtw \in \R^p, b \in \R\right\}.
    $$ 
  \end{small}
  \item Both logistic regression and linear regression are subspaces of $\Hspace$ (if $\tau$ is the logistic sigmoid / identity function).  
\end{itemize}

\vspace*{-0.48cm}

\begin{figure}
  \centering
    \scalebox{0.9}{\includegraphics{plots/neuron_regcls.png}}
    \vspace*{-0.2cm}
    \begin{tiny}
    \caption{\textit{Left}: A regression line learned by a single neuron. \textit{Right}: A decision-boundary learned by a single neuron in a binary classification task.}
    \end{tiny}
\end{figure}

\end{vbframe}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe} {A Single Neuron: Optimization}
\begin{itemize}
\item To optimize this model, we minimize the empirical risk 
$$\riske = \sumin \Lxyi,$$
where $\Lxy$ is a loss function. It compares the network's predictions $\fx$ to the ground truth $y$. 
\item For regression, we typically use the L2 loss (rarely L1): $$\Lxy = \frac{1}{2}(y - \fx)^2$$
\item For binary classification, we typically apply the cross entropy loss (also known as bernoulli loss): $$\Lxy = y \log \fx + (1 - y) \log(1 - \fx)$$
    
\framebreak 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\item For a single neuron, in both cases, the loss function is convex and the global optimum can be found with an iterative algorithm like gradient descent. 

\item In fact, a single neuron with logistic sigmoid function trained with the bernoulli loss does not only have the same hypothesis space as a logistic regression and is therefore the same model, but will also yield to the very same result when trained until convergence.
\end{itemize}
\end{vbframe} 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\endlecture