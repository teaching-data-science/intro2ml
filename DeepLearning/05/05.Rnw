% Introduction to Machine Learning
% Day 4 xxxx

% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../../style/preamble.Rnw")
# library(rpart) xxxx
# library(rpart.plot)
# library(randomForest)
# library(rattle)
# library(smoof)
@

% Load all R packages and set up knitr
<<setup, child="../../style/setup.Rnw", include = FALSE>>=
@

% Defines macros and environments
\input{../../latex-math/basic-ml.tex}

%! includes: cart-stoppingpruning xxxx

\lecturechapter{Deep Learning: CNNs}
\lecture{Introduction to Machine Learning}

\sloppy

\begin{vbframe}{Convolutional neural networks}
  \begin{itemize}
    \item Convolutional Neural Networks (CNNs) are probably the most important model class in the world of deep learning.
    \item Since 2012, they are state-of-the-art for many machine vision tasks.
    \item Common applications include:
    \begin{itemize}
      \item Image classification.
      \item Object localization.
      \item Image segmentation.
    \end{itemize}
    \item Also widely applied in other domains such as text, audio and even time-series data.
    \item Basic idea: a CNN automatically extracts visual, or, more generally, spatial features from an input such that it is able to make the optimal prediction based on those features.
    \item Therefore, it contains different building blocks...
  \end{itemize}
\end{vbframe}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{CNNs - What for?}
  \begin{figure}
    \centering
    \includegraphics[width=11cm]{figure_man/cnn/01_introduction/translate.png}
    \caption{Automatic Machine Translation (Otavio Good (2015)). The Google Translate app does real-time visual translation of more than 20 languages. A CNN is used to recognize the characters on the image and a recurrent neural network (RNN) for the translation.}
  \end{figure}
\framebreak
  \begin{figure}
    \centering
    \includegraphics[width=11cm]{figure_man/cnn/01_introduction/driving.png}
    \caption{End to End Learning for Self-Driving Cars (Mariusz Bojarski et al. (2016)). A convolutional neural network is used to map raw pixels from a single front-facing camera directly into steering commands. With minimum training data from humans, the system learns to drive in traffic on local roads with or without lane markings and on highways.}
  \end{figure}
\framebreak
  \begin{figure}
    \centering
    \includegraphics[width=11cm]{figure_man/cnn/01_introduction/colorization.png}
    \caption{Colorful Image Colorization (Zhang et al. (2016)). Given a grayscale photograph as input (top row), this network attacks the problem of hallucinating a plausible color version of the photograph (bottom row, i.e. the prediction of the network). Realizing this task manually consumes many hours of time.}
  \end{figure}
\framebreak
  \begin{figure}
    \centering
    \includegraphics[width=9cm]{figure_man/cnn/01_introduction/segmentation.png}
    \caption{Image segmentation (Hyeonwoo Noh et al. (2013)). The neural network is composed of deconvolution (the transpose of a convolution) and unpooling layers, which identify pixel-wise class labels and predict segmentation masks.}
  \end{figure}
\framebreak
  \begin{figure}
    \centering
    \includegraphics[width=6cm]{figure_man/cnn/01_introduction/road_seg.png}
    \caption{Road segmentation (Mnih Volodymyr (2013)). Aerial images and possibly outdated map pixels are labeled.}
  \end{figure}
\framebreak
  \begin{figure}
    \centering
    \includegraphics[width=10cm]{figure_man/cnn/01_introduction/speech.png}
    \caption{Speech recognition (Anand \& Verma (2015)). Convolutional neural network to extract features from audio data in order to classify emotions.}
  \end{figure}
\framebreak
  \begin{figure}
    \centering
    \includegraphics[width=8cm]{figure_man/cnn/01_introduction/object_localization.png}
    \caption{Object localization with Mask R-CNN (He (2017)). Convolutional neural network to localize and segment objects in images.}
  \end{figure}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
\frametitle{CNNs - A First Glimpse}
 \center
    \only<1>{\includegraphics[width=9cm]{figure_man/cnn/01_introduction/alexnet.png}}%
    \only<2>{\includegraphics[width=9cm]{figure_man/cnn/01_introduction/alexnet.png}}%
    \begin{itemize}
        \only<1>{\item Input layer: contains the input (-image) as data matrices.}
        \only<1>{\item \textbf{Convolutions}: extract feature maps from a previous layer.}
        \only<1>{\item \textbf{Pooling}: reduces the dimensionality of any input and filter robust features.}
        \only<2>{\item Fully connected layer: standard layer that connects feature map elements with the output neurons.}
        \only<2>{\item Softmax: squashes output values to probability scores.}
    \end{itemize}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Convolutions --- Basic Idea}

\begin{vbframe}{Filters to extract features}
    \begin{itemize}
        \item Filters are widely applied in Computer Vision (CV) since the 70's.
        \item One prominent example: \textbf{Sobel-Filter}.
        \item Detects edges in images.
    \end{itemize}
    \begin{figure}
        \centering
        \includegraphics[width=10cm]{figure_man/cnn/02_filters/sobel_einstein.png}
        % source: https://github.com/qmegas/sobel-operator/blob/HEAD/images/1.jpg
        \caption{Sobel-filtered image.}
    \end{figure}
\framebreak
    \begin{itemize}
        \item Edges occur where the intensity over neighboring pixels changes fast.
        \item Thus, approximate the gradient of the intensity of each pixel.
        \item Sobel showed that the gradient image $G_x$ of original image $A$ in x-dimension  can be approximated by:
        $$
            G_x = 
            \begin{bmatrix}
                -1 & 0 & +1 \\
                -2 & 0 & +2 \\
                -1 & 0 & +1 
            \end{bmatrix} * A = S_x * A
        $$
        where $*$ indicates a mathematical operation known as a \textbf{convolution}, not a traditional matrix multiplication.
        \item The filter matrix $S_x$ consists of the product of an \textbf{averaging} and a \textbf{differentiation} kernel: 
        $$
            \underbrace{\begin{bmatrix}
                1 & 2 & 1   
            \end{bmatrix}^{T}}_{averaging}
        % $$
        % $$
            \underbrace{\begin{bmatrix}
                -1 & 0 & +1   
            \end{bmatrix}}_{differentiation}
        $$
        
        % \footnotesize{(Note :  More on this later.)}
    \end{itemize}
\framebreak
    \begin{itemize}
        \item Similarly, the gradient image $G_y$ in y-dimension can be approximated by:
        $$
            G_y = 
            \begin{bmatrix}
                -1 & -2 & -1  \\
                0 & 0 & 0 \\
                +1 & +2 & +1
            \end{bmatrix} * A = S_y * A
        $$
        \item The combination of both gradient images yields a dimension-independent gradient information $G$:
        $$
            G = \sqrt{G_x^2 + G_y^2}
        $$
        \item These matrix operations were used to create the filtered picture of Albert Einstein.
    \end{itemize}
\end{vbframe}

\begin{frame}{Horizontal vs Vertical edges}

    \begin{figure}
        \centering
          \scalebox{0.9}{\includegraphics{figure_man/cnn/02_filters/sobel_bike.png}}
          \tiny{\\ Source: Wikipedia}
         \caption{\footnotesize{Sobel filtered images. Outputs are normalized in each case. }}
    \end{figure}

\end{frame}

\frame{
\frametitle{Filters to extract features}
    \center
    \only<1>{\includegraphics[width=11cm]{figure_man/cnn/02_filters/sobel1.png}}%
    \only<2>{\includegraphics[width=11cm]{figure_man/cnn/02_filters/sobel2.png}}%
    \only<3>{\includegraphics[width=11cm]{figure_man/cnn/02_filters/sobel3.png}}%
    \only<4>{\includegraphics[width=11cm]{figure_man/cnn/02_filters/sobel4.png}}%
    \only<5>{\includegraphics[width=11cm]{figure_man/cnn/02_filters/sobel4.png}}%
    %\only<6>{\includegraphics[width=11cm]{figure_man/cnn/02_filters/sobel5.png}}%
    \only<6>{\includegraphics[width=6cm]{figure_man/cnn/02_filters/sobel6.png}}%
    \only<7>{\includegraphics[width=11cm]{figure_man/cnn/02_filters/sobel8.png}}%

    \begin{itemize}
        \only<1>{\item Let's do this on a dummy image.}
        \only<1>{\item How to represent a digital image?}
        \only<2>{\item Basically as an array of integers.}
        \only<3>{\item $S_x$ enables us to to detect vertical edges!}
        \only<4>{\item[]}
        \only<5>{\item[]
        \vspace{-0.8cm}
        \begin{alignat*}{3}
            (\mathit{G}_x)_{(i,j)} = (I \star S_x)_{(i, j)}
                 & = -1 \cdot 0 \ \ &&+ \ \ 0 \cdot 255 \ \ &&+ \ \ \textbf{1} \cdot \textbf{255} \\
                 &\quad - 2 \cdot 0 &&+ \ \ 0 \cdot 0 &&+ \ \ \textbf{2} \cdot \textbf{255} \\
                 &\quad - 1 \cdot 0 &&+ \ \ 0 \cdot 255 &&+ \ \ \textbf{1} \cdot \textbf{255}
                 \notag
        \end{alignat*}
        }
        % \only<6>{\item[] \textcolor{white}{Applying the Sobel-Operator to every location in the input yields us the \textbf{feature map}.}}
        \only<6>{\item Applying the Sobel-Operator to every location in the input yields us the \textbf{feature map}.}
        \only<7>{\item Normalized feature map reveals vertical edges.
        \item Note the dimensional reduction compared to the dummy image.}
    \end{itemize}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Why do we need to know all of that?}
  \begin{itemize}
    \item What we just did was extracting \textbf{pre-defined} features from our input (i.e. edges).
    \item A convolutional neural network does almost exactly the same: \enquote{extracting features from the input}.
    \item[] $\Rightarrow$ The main difference is that we usually do not tell the CNN what to look for (pre-define them), \textbf{the CNN decides itself}.
    \item In a nutshell:
    \begin{itemize}
      \item We initialize a lot of random filters (like the Sobel but just random entries) and apply them to our input.
      \item Then, a classifier which is generally a feed forward neural net, uses them as input data.
      \item Filter entries will be adjusted by common gradient descent methods.
    \end{itemize}
  \end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{

\frametitle{Why do we need to know all of that?}
    \center
    \only<1>{\includegraphics[width=11cm]{figure_man/cnn/02_filters/sobel9.png}}%
    \only<2>{\includegraphics[width=10.5cm]{figure_man/cnn/02_filters/sobel10.png}}%
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Working with images}
  \begin{itemize}
  \item In order to understand the functionality of CNNs, we have to familiarize ourselves with some properties of images.
  \item Grey scale images:
  \begin{itemize}
    \item Matrix with dimensions \textbf{h}eight $\times$ \textbf{w}idth $\times$ 1.
    \item Pixel entries differ from 0 (black) to 255 (white).
  \end{itemize}
  \item Color images:
  \begin{itemize}
    \item Tensor with dimensions \textbf{h}eight $\times$ \textbf{w}idth $\times$ 3.
    \item The depth 3 denotes the RGB values (red - green - blue).
  \end{itemize}
  \item Filters:
  \begin{itemize}
    \item A filter's depth is \textbf{always} equal to the input's depth!
    \item In practice, filters are usually quadratic.
    \item Thus we only need one integer to define its size.
    \item For example, a filter of size $2$ applied on a color image actually has the dimensions $2 \times 2 \times 3$.
  \end{itemize}
  \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{

\frametitle{The 2d convolution}

  \begin{itemize}

    \only<1-8>{\item Suppose we have an input with entries $a, b, \dots, i$ (think of pixel values).}
    \only<1-8>{\item The filter we would like to apply has weights $w_{11}, w_{12}, w_{21} \text{ and } w_{22}$.}

  \end{itemize}

  \center
  \only<1>{\includegraphics[width=8cm]{figure_man/cnn/04_conv2d/2dconv1.png}}%
  \only<2>{\includegraphics[width=8cm]{figure_man/cnn/04_conv2d/2dconv2.png}}%
  \only<3>{\includegraphics[width=8cm]{figure_man/cnn/04_conv2d/2dconv2.png}}%
  \only<4>{\includegraphics[width=8cm]{figure_man/cnn/04_conv2d/2dconv3.png}}%
  \only<5>{\includegraphics[width=8cm]{figure_man/cnn/04_conv2d/2dconv4.png}}%
  \only<6>{\includegraphics[width=8cm]{figure_man/cnn/04_conv2d/2dconv5.png}}%
  \only<7>{\includegraphics[width=8cm]{figure_man/cnn/04_conv2d/2dconv6.png}}%
  \only<8>{\includegraphics[width=8cm]{figure_man/cnn/04_conv2d/2dconv6.png}}%

  \begin{itemize}

    \only<1>{\item[] }
    \only<2>{\item[] }
    \only<3>{\item[] To obtain $s_{11}$ we simply compute the dot product:}
    \only<3>{\item[] $s_{11} = a \cdot w_{11} + b \cdot w_{12} + d \cdot w_{21} + e \cdot w_{22}$}
    \only<4>{\item[] Same for $s_{12}$:}
    \only<4>{\item[] $s_{12} = b \cdot w_{11} + c \cdot w_{12} + e \cdot w_{21} + f \cdot w_{22}$}
    \only<5>{\item[] As well as for $s_{21}$:}
    \only<5>{\item[] $s_{21} = d \cdot w_{11} + e \cdot w_{12} + g \cdot w_{21} + h \cdot w_{22}$}
    \only<6>{\item[] And finally for $s_{22}$:}
    \only<6>{\item[] $s_{22} = e \cdot w_{11} + f \cdot w_{12} + h \cdot w_{21} + i \cdot w_{22}$}
    \only<7>{\item[] $s_{11} = a \cdot w_{11} + b \cdot w_{12} + d \cdot w_{21} + e \cdot w_{22}$}
    \only<7>{\item[] $s_{12} = b \cdot w_{11} + c \cdot w_{12} + e \cdot w_{21} + f \cdot w_{22}$}
    \only<7>{\item[] $s_{21} = d \cdot w_{11} + e \cdot w_{12} + g \cdot w_{21} + h \cdot w_{22}$}
    \only<7>{\item[] $s_{22} = e \cdot w_{11} + f \cdot w_{12} + h \cdot w_{21} + i \cdot w_{22}$}
	\only<8>{\item[] More generally, let $I$ be the matrix representing the input and $W$ be the filter/kernel. Then the entries of the output matrix are defined by $s_{ij} = \sum_{m,n} I_{i+m-1, j+n-1} w_{mn}$}

  \end{itemize}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Network properties induced by convolution}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\frame{

\frametitle{Sparse interactions}

  \center
  \only<1>{\includegraphics[width=7cm]{figure_man/cnn/06_conv_properties/sparse/sparse0.png}}%
  \only<2>{\includegraphics[width=7cm]{figure_man/cnn/06_conv_properties/sparse/sparse1.png}}%
  \only<3>{\includegraphics[width=7cm]{figure_man/cnn/06_conv_properties/sparse/sparse2.png}}%
  \only<4>{\includegraphics[width=7cm]{figure_man/cnn/06_conv_properties/sparse/sparse3.png}}%
  \only<5>{\includegraphics[width=7cm]{figure_man/cnn/06_conv_properties/sparse/sparse4.png}}%
  \only<6>{\includegraphics[width=7cm]{figure_man/cnn/06_conv_properties/sparse/sparse5.png}}%
  \only<7>{\includegraphics[width=7cm]{figure_man/cnn/06_conv_properties/sparse/sparse6.png}}%
  \only<8>{\includegraphics[width=7cm]{figure_man/cnn/06_conv_properties/sparse/dense0.png}}%
  \only<9>{\includegraphics[width=7cm]{figure_man/cnn/06_conv_properties/sparse/dense1.png}}%
  \only<10>{\includegraphics[width=7cm]{figure_man/cnn/06_conv_properties/sparse/dense2.png}}%

  \begin{itemize}

    \only<1>{\item We want to use the \enquote{neuron-wise} representation of our CNN.}
    \only<2>{\item Moving the filter to the first spatial location...}
    \only<3>{\item ...yields us the first entry of the feature map...}
    \only<4>{\item ...which is composed of these four connections.}
    \only<5>{\item $s_{12}$ is composed by these four connections.}
    \only<6>{\item $s_{21}$ by these..}
    \only<7>{\item and finally $s_{22}$ by these.}
    \only<8>{\item Assume we would replicate the architecture with a dense net.}
    \only<9>{\item Each input neuron is connected with each hidden layer neuron.}
    \only<10>{\item In total, we obtain 36 connections!}

  \end{itemize}

}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Sparse interactions}
  \begin{itemize}
    \item What does that mean?
    \begin{itemize}
      \item Our CNN has a \textbf{receptive field} of 4 neurons.
      \item That means, we apply a \enquote{local search} for features.
      \item A dense net on the other hand conducts a \enquote{global search}.
      \item The receptive field of the dense net are 9 neurons.
    \end{itemize}
    \item When processing images, it is more likely that features occur at specific locations in the input space.
    \item For example, it is more likely to find the eyes of a human in a certain area, like the face.
    \begin{itemize}
      \item A CNN only incorporates the surrounding area of the filter into its feature extraction process.
      \item The dense architecture on the other hand assumes that every single pixel entry has an influence on the eye, even pixels far away or in the background.
    \end{itemize}
  \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\frame{

\frametitle{Parameter Sharing}

  \center
  \only<1>{\includegraphics[width=7cm]{figure_man/cnn/06_conv_properties/ps/ps0.png}}%
  \only<2>{\includegraphics[width=7cm]{figure_man/cnn/06_conv_properties/ps/ps1.png}}%
  \only<3>{\includegraphics[width=7cm]{figure_man/cnn/06_conv_properties/ps/ps3.png}}%
  \only<4>{\includegraphics[width=7cm]{figure_man/cnn/06_conv_properties/ps/ps4.png}}%
  \only<5>{\includegraphics[width=7cm]{figure_man/cnn/06_conv_properties/ps/ps5.png}}%
  \only<6>{\includegraphics[width=7cm]{figure_man/cnn/06_conv_properties/ps/ps6.png}}%
  \only<7>{\includegraphics[width=7cm]{figure_man/cnn/06_conv_properties/ps/ps7.png}}%
  \only<8>{\includegraphics[width=7cm]{figure_man/cnn/06_conv_properties/ps/ps8.png}}%
  \only<9>{\includegraphics[width=7cm]{figure_man/cnn/06_conv_properties/ps/ps9.png}}%
  \only<10>{\includegraphics[width=7cm]{figure_man/cnn/06_conv_properties/ps/dense0.png}}%
  \only<11>{\includegraphics[width=7cm]{figure_man/cnn/06_conv_properties/ps/dense1.png}}%

  \begin{itemize}

    \only<1>{\item For the next property we focus on the filter entries.}
    \only<2>{\item In particular, we consider weight $w_{11}$}
    \only<3>{\item As we move the filter to the first spatial location..}
    \only<4>{\item ...we observe the following connection for weight $w_{11}$}
    \only<5>{\item Moving to the next location...}
    \only<6>{\item ...highlights that we use the same weight more than once!}
    \only<7>{\item Even three...}
    \only<8>{\item And in total four times.}
    \only<9>{\item All together, we have just used four weights.}
    \only<10>{\item How many weights does a corresponding dense net use?}
    \only<11>{\item $9 \cdot 4 = 36$! That is 9 times more weights!}

  \end{itemize}

}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Sparse Connections and Parameter sharing}
  \begin{itemize}
    \item Why is that good?
    \item Less parameters drastically reduce memory requirements.
    \item Faster runtime:
    \begin{itemize}
      \item For $m$ inputs and $n$ outputs, a fully connected layer requires $m\times n$ parameters and has $\mathcal{O}(m\times n)$ runtime.
      \item A convolutional layer has limited connections $k<<m$, thus only $k\times n$ parameters and $\mathcal{O}(k\times n)$ runtime.
    \end{itemize}
    \item But it gets even better:
    \begin{itemize}
      \item Less parameters mean less overfitting and better generalization!
    \end{itemize}
  \end{itemize}
\framebreak
  \begin{itemize}
    \item Example: consider a color image with size $100 \times 100$.
    \item Suppose we would like to create one single feature map with a \enquote{same padding} (i.e. the hidden layer is of the same size).
    %(i.e. retain the dim of the input for our feature maps).
    \begin{itemize}
      \item Choosing a filter with size $5$ means that we have a total of $5 \cdot 5 \cdot 3 = 75$ parameters (bias unconsidered).
      \item A dense net with the same amount of \enquote{neurons} in the hidden layer results in 
      $$\underbrace{(100^2 \cdot 3)}_{\text{input}} \cdot \underbrace{(100^2)}_{\text{hidden layer}} = 300.000.000 $$ parameters.
      
      %\item A dense net needs $10.000$ neurons in its hidden layer to replicate that architecture ($100 \cdot 100 = 10.000$). It has $100 \cdot 100 \cdot 3 \cdot 10.000 = 300.000.000$ parameters (bias unconsidered)!
      
    \end{itemize}
  \item Note that this was just a fictitious example. In practice we do not try to replicate CNN architectures with dense networks (actually it isn't even possible since physical limitations like the computer hardware would not allow us to).
  \end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\frame{

\frametitle{Equivariance to translation}

  \center
  \only<1>{\includegraphics[width=7cm]{figure_man/cnn/06_conv_properties/equivariance/equi0.png}}%
  \only<2>{\includegraphics[width=7cm]{figure_man/cnn/06_conv_properties/equivariance/equi1.png}}%
  \only<3>{\includegraphics[width=7cm]{figure_man/cnn/06_conv_properties/equivariance/equi2.png}}%
  \only<4>{\includegraphics[width=7cm]{figure_man/cnn/06_conv_properties/equivariance/equi3.png}}%

  \begin{itemize}

    \only<1>{\item Think of a specific feature of interest, here highlighted in grey.}
    \only<2>{\item Furthermore, assume we had a tuned filter looking for exactly that feature.}
    \only<3>{\item The filter does not care at what location the feature of interest is located at.}
    \only<4>{\item It is literally able to find it anywhere! That property is called \textbf{equivariance to translation}. \\[0.2cm] \scriptsize{Note: A function $f(x)$ is equivariant to a function $g$ if $f(g(x)) = g(f(x))$.}}
  \end{itemize}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Nonlinearity in feature maps}
    \begin{itemize}
        \item As in dense nets, we use activation functions on all feature map entries to introduce nonlinearity in the net.
        \item Typically rectified linear units (ReLU) are used in CNNs:
            \begin{itemize}
                \item They reduce the danger of saturating gradients compared to sigmoid activations.
                \item They can lead to \textit{sparse activations}, as neurons $\leq 0$ are squashed to $0$ which increases computational speed.
            \end{itemize}
        \item As seen in the last chapter, many variants of ReLU (Leaky ReLU, ELU, PReLU, etc.) exist.
    \end{itemize}
\end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Convolutions --- Mathematical Perspective}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{vbframe}{Convolutions: A Deeper Look}
%     \begin{itemize}
%         \item CNNs borrow their name from a mathematical operation termed \textbf{convolution} that originates in Signal Processing.
%         \item Basic understanding of this concept and related operations improves the understanding of the CNN functionality.
%         \item Still, there are successful practitioners that never heard of these concepts.
%         \item The following should provide exactly this fundamental understanding of convolutions.
%     \end{itemize}
% \framebreak
%     \begin{itemize}
%         \item Definition: \\
%             \begin{equation*}
%                 \begin{split}
%                     h(i) &= (f \ast g)(i) = \int_{-\infty}^{\infty} f(x)g(i-x)dx \\
%                     \text{where } f(x)& \text{ input function} \\
%                     \text{and } g(x)& \text{ weighting function, kernel} \\
%                     \text{and } h(i)& \text{ output function, feature map elements} \\
%                     \text{for } f, g &\in \R^{d} \mapsto h \in \R^{d}
%                 \end{split}
%             \end{equation*}
%         \item Intuition 1: Weighted smoothing of $f(x)$ with weighting function $g(x)$. 
%         \item Intuition 2: Filter function $g(x)$ filters features $h(i)$ from input signal $f(x)$.
%     \end{itemize}
% \framebreak
%     \begin{itemize}
%         \item Discretization for $\R^1$: \\
%             \begin{equation*}
%                 \begin{split}
%                     h(i) &= (f \ast g)(i) = \sum_{x = -\infty}^{\infty} f(x)g(i-x)
%                 \end{split}
%             \end{equation*}
%         \item Discretization for 2D images:
%             \begin{itemize}
%                 \item $\mathcal{I} \in \R^2$ contains two dimensions
%                 \item Use 2D Kernel $\mathcal{G}$ to yield feature map $\mathcal{H}$:
%                 \begin{equation*}
%                     \begin{split}
%                         \mathcal{H}(i, j) &= (\mathcal{I} \ast \mathcal{G})(i, j) = \sum_{x} \sum_{y} \mathcal{I}(x, y) \mathcal{G}(i-x, j-y) \\
%                         \text{where } x, y &:= \text{indices $\mathcal{I}$ and $\mathcal{G}$} \\
%                         \text{and } i, j &:= \text{indices elements in } \mathcal{H} \\
%                         \end{split}
%                 \end{equation*}
%             \end{itemize}
%     \end{itemize}
% \end{vbframe}    
% 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{vbframe}{1D Convolution Animation}
%     \begin{figure}
%     \centering
%     \includegraphics[width=10cm]{figure_man/cnn/conv_animations/conv_static.png}
%     \end{figure}
%     \begin{equation*}
%         f(x)=
%         \begin{cases}
%             1, & \text{if } x\in [-1, 2]\\
%             0, & \text{otherwise}
%         \end{cases}
%         \qquad
%         g(x)=
%         \begin{cases}
%             1 - 0.2*|x|, & \text{if } x > 0\\
%             0, & \text{otherwise}
%         \end{cases}
%     \end{equation*}
% \end{vbframe}
% 
% \begin{vbframe}{1D Convolution Animation}
%     \begin{figure}
%     \centering
%     \includegraphics[width=10cm]{figure_man/cnn/conv_animations/conv_static.png}
%     \end{figure}
%     Kernel is flipped due to the negative iterator in $$h(i) = \sum_{x = -\infty}^{\infty} f(x)g(i-x)$$
% \end{vbframe}
% 
% % \frame{
% % \frametitle{1D Convolution Animation}
% %     \center
% %         \only<1>{\includegraphics[width=11cm]{figure_man/cnn/conv_animations/conv_anim_1.png}}%
% %         \only<2>{\includegraphics[width=11cm]{figure_man/cnn/conv_animations/conv_anim_2.png}}%
% %         \only<3>{\includegraphics[width=11cm]{figure_man/cnn/conv_animations/conv_anim_3.png}}%
% %         \only<4>{\includegraphics[width=11cm]{figure_man/cnn/conv_animations/conv_anim_4.png}}%
% %         \only<5>{\includegraphics[width=11cm]{figure_man/cnn/conv_animations/conv_anim_5.png}}%
% %         \only<6>{\includegraphics[width=11cm]{figure_man/cnn/conv_animations/conv_anim_6.png}}%
% %         \only<7>{\includegraphics[width=11cm]{figure_man/cnn/conv_animations/conv_anim_7.png}}%
% %         \only<8>{\includegraphics[width=11cm]{figure_man/cnn/conv_animations/conv_anim_8.png}}%
% %         \only<9>{\includegraphics[width=11cm]{figure_man/cnn/conv_animations/conv_anim_9.png}}%
% %         \only<10>{\includegraphics[width=11cm]{figure_man/cnn/conv_animations/conv_anim_10.png}}%
% %         \only<11>{\includegraphics[width=11cm]{figure_man/cnn/conv_animations/conv_anim_11.png}}%
% %         \only<12>{\includegraphics[width=11cm]{figure_man/cnn/conv_animations/conv_anim_12.png}}%
% %         \only<13>{\includegraphics[width=11cm]{figure_man/cnn/conv_animations/conv_anim_13.png}}%
% %         \only<14>{\includegraphics[width=11cm]{figure_man/cnn/conv_animations/conv_anim_14.png}}%
% %         \only<15>{\includegraphics[width=11cm]{figure_man/cnn/conv_animations/conv_anim_15.png}}%
% %         \only<16>{\includegraphics[width=11cm]{figure_man/cnn/conv_animations/conv_anim_16.png}}%
% %         \only<17>{\includegraphics[width=11cm]{figure_man/cnn/conv_animations/conv_anim_17.png}}%
% %         \only<18>{\includegraphics[width=11cm]{figure_man/cnn/conv_animations/conv_anim_18.png}}%
% %         \only<19>{\includegraphics[width=11cm]{figure_man/cnn/conv_animations/conv_anim_19.png}}%
% %         \only<20>{\includegraphics[width=11cm]{figure_man/cnn/conv_animations/conv_anim_20.png}}%
% %         \only<21>{\includegraphics[width=11cm]{figure_man/cnn/conv_animations/conv_anim_21.png}}%
% %         \only<22>{\includegraphics[width=11cm]{figure_man/cnn/conv_animations/conv_anim_22.png}}%
% %         \only<23>{\includegraphics[width=11cm]{figure_man/cnn/conv_animations/conv_anim_23.png}}%
% %         \only<24>{\includegraphics[width=11cm]{figure_man/cnn/conv_animations/conv_anim_24.png}}%
% %         \only<25>{\includegraphics[width=11cm]{figure_man/cnn/conv_animations/conv_anim_25.png}}%
% %         \only<26>{\includegraphics[width=11cm]{figure_man/cnn/conv_animations/conv_anim_26.png}}%
% %         \only<27>{\includegraphics[width=11cm]{figure_man/cnn/conv_animations/conv_anim_27.png}}%
% %         \only<28>{\includegraphics[width=11cm]{figure_man/cnn/conv_animations/conv_anim_28.png}}%
% %         \only<29>{\includegraphics[width=11cm]{figure_man/cnn/conv_animations/conv_anim_29.png}}%
% % }
% 
% 
% \frame{
% \frametitle{1D Convolution Animation}
%     \center
%         \only<1>{\includegraphics[width=11cm]{figure_man/cnn/conv_animations/conv_anim_3.png}}%
%         \only<2>{\includegraphics[width=11cm]{figure_man/cnn/conv_animations/conv_anim_10.png}}%
%         \only<3>{\includegraphics[width=11cm]{figure_man/cnn/conv_animations/conv_anim_15.png}}%
%         \only<4>{\includegraphics[width=11cm]{figure_man/cnn/conv_animations/conv_anim_19.png}}%
%         \only<5>{\includegraphics[width=11cm]{figure_man/cnn/conv_animations/conv_anim_24.png}}%
%         \only<6>{\includegraphics[width=11cm]{figure_man/cnn/conv_animations/conv_anim_28.png}}%
% }
% 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{vbframe}{Properties of the Convolution}
%     \begin{itemize}
%         \item Commutativity:
%         $$ f \ast g = g \ast f$$
%         \item Associativity:
%         $$ (f \ast g) \ast h = f \ast (g \ast h)$$
%         \item Distributivity:
%         $$ f \ast (g + h) = f\ast g + f \ast h$$ 
%         $$ \alpha (f \ast g) = (\alpha f) \ast g \text{ for scalar } \alpha$$ 
%         \item Differentiability:
%         $$ \frac{\partial (f\ast g)(x)}{\partial x_i} = \frac{\partial f(x)}{\partial x_i}\ast g(x) = \frac{\partial g(x)}{\partial x_i} \ast f(x)$$ \\
%         $ \rightarrow (f\ast g)(x)$ is as many times differentiable as the max of $g(x)$ and $f(x)$.
%     \end{itemize}
% \end{vbframe}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 
% \begin{vbframe}{Related operations}
%     \begin{itemize}
%         \item Convolution is strongly related to two other mathematical operators:
%         \begin{enumerate}
%             \item Fourier transform via the Convolution Theorem
%             \item Cross-correlation
%         \end{enumerate}
%     \end{itemize}
% \end{vbframe}
% 
% % nice explanation here: https://www.quora.com/How-are-neural-networks-related-to-Fourier-transforms
% \begin{vbframe}{Convolution Theorem}
%     \begin{itemize}
%         \item  Fourier transform of the convolution of two functions can be expressed as the product of their Fourier transforms:
%         $$ 
%             \mathcal{F} \{ f\ast g\} = \mathcal{F} \{f\}\mathcal{F} \{g\}
%         $$
%         \item Transformation of a signal from time to frequency domain.
%         \item Convolution in the time domain is equivalent to multiplication in frequency domain.
%         \item The computationally fastest way to compute a convolution is therefore taking the Fourier inverse of the multiplication of the Fourier-transformed input and filter function:
%         $$
%             (f \ast g)(t) = \mathcal{F}^{-1}\{\mathcal{F} \{f(t)\} \mathcal{F} \{g(t)\}\}
%         $$
%     \end{itemize}
% \end{vbframe}
% 
% % \begin{vbframe}{Convolution Theorem - Proof}
% %     \begin{equation*}
% %         \begin{split}
% %             \widehat{(f \ast g)(t)} &= \int_{-\infty}^\infty exp(-2 \pi i \omega t) \Big[ \int_{-\infty}^\infty f(\tau) g(t-\tau)d\tau \Big] \\
% %             & = \int_{-\infty}^\infty \int_{-\infty}^\infty exp(-2 \pi i \omega t) f(\tau)g(t-\tau)d\tau dt \\
% %             & \overset{Fubini}{=} \int_{-\infty}^\infty \Big[ \int_{-\infty}^\infty exp(-2 \pi i \omega t) f(\tau) g(t-\tau ) dt  \Big] d\tau \\
% %             & \overset{f(\tau) \perp t}{=} \int_{-\infty}^\infty f(\tau) \Big[ \int_{-\infty}^\infty exp(-2 \pi i \omega t) g(t-\tau) dt \Big]d \tau \\
% %             & \overset{u = t - \tau}{=} \int_{-\infty}^\infty f(\tau) \Big[ \int_{-\infty}^\infty exp(-2\pi i \omega \tau) exp(-2 \pi i \omega u) g(u) du \Big] d\tau \\
% %             & = \int_{-\infty}^\infty exp(-2 \pi i \omega \tau) f(\tau) \Big[ \int_{-\infty}^\infty exp(-2 \pi i \omega u) g(u) du \Big] d \tau \\
% %             & \overset{Fubini}{=} ...
% %         \end{split}
% %     \end{equation*}
% % \end{vbframe}
% % 
% % \begin{vbframe}{Convolution Theorem - Proof}
% %     \begin{equation*}
% %         \begin{split}
% %             &... \int_{-\infty}^\infty exp(-2 \pi i \omega \tau) f(\tau) d\tau \int_{-\infty}^\infty exp(-2 \pi i \omega u) g(u) du \\
% %             &= \hat{f(t)}\hat{g(t)}
% %         \end{split}
% %     \end{equation*}
% % \end{vbframe}
% 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % nice xcor vs conv video by caltech
% % https://www.youtube.com/watch?v=MQm6ZP1F6ms
% 
% \begin{vbframe}{Cross-correlation}
%     \begin{itemize}
%         \item Measurement for similarity of two functions $f(x), g(x)$.
%         \item More specifically, at which position are the two functions most similar to each other? Where does the pattern of $g(x)$ match $f(x)$ the best?
%         \item Intuition:
%         \begin{itemize}
%                 \item Slide with $g(x)$ over $f(x)$ and at each discrete step compute the sum of the product of their elements.
%                 \item When peaks of both functions are aligned, the product of high (positive or negative) values will lead to high sums.
%                 \item Thus, both functions are most similar at points with equal peaks.
%         \end{itemize}
%     \end{itemize}
% \framebreak
%     \begin{itemize}
%         \item Definition: \\
%             \begin{equation*}
%                 \begin{split}
%                     h(i) &= (f \star g)(i) = \int_{-\infty}^{\infty} f(x)g(i+x)dx \\
%                     \text{where } f(x)& \text{ input function} \\
%                     \text{and } g(x)& \text{ weighting function, kernel} \\
%                     \text{and } h(i)& \text{ output function, feature map elements} \\
%                     \text{for } f, g &\in \R^{d} \mapsto h \in \R^{d}
%                 \end{split}
%             \end{equation*}
%     \end{itemize}
% \framebreak 
%     \begin{itemize}
%         \item Discrete formulation:
%             \begin{equation*}
%                 h(i) = (f \star g)(i) = \sum_{x = -\infty}^{\infty} f(x)g(i+x)
%             \end{equation*}
%         \item Thus:
%             \begin{equation*}
%                     f(i) \star g(i) = f(-i) \ast g(i)
%             \end{equation*}
%         \item Remember: $\ast$ is used for convolution and $\star$ for cross-correlation.
%         \item Similar formulation as the convolution despite the flipped filter function in the convolutional kernel.
%     \end{itemize}
% \framebreak
%     \begin{itemize}
%         \item This operation also works in 2 dimensions
%         \item The difference w.r.t. the convolution are the positive iterators in the sum:
%         \begin{equation*}
%                     \begin{split}
%                         \mathcal{H}(i, j) &= (\mathcal{I} \ast \mathcal{G})(i, j) = \sum_{x} \sum_{y} \mathcal{I}(x, y) \mathcal{G}(i+x, j+y) \\
%                         \text{where } x, y &:= \text{indices $\mathcal{I}$ and $\mathcal{G}$} \\
%                         \text{and } i, j &:= \text{indices elements in } \mathcal{H} \\
%                         \end{split}
%                 \end{equation*}
%     \end{itemize}
% \framebreak
%     \begin{figure}
%         \centering
%         \includegraphics[width=8.5cm]{figure_man/cnn/other/template_match.png}
%         \caption{Cross-correlation used to detect a template (onion) in an image. Cross-correlation peaks (white) at the position where template and input match best.}
%     \end{figure}
% \end{vbframe}
% 
% \begin{vbframe}{Cross-Correlation}
%     % nice video
%     \begin{itemize}
%         \item From the following animation we see that
%         \begin{itemize}
%             \item Kernel is not flipped as opposed to the convolution.
%             \item Cross-correlation peaks, where the filter matches the signal the most.    
%         \end{itemize}
%         \item In some frameworks, cross-correlation is implemented instead of the convolution due to
%         \begin{itemize}
%             \item better computational performance.
%             \item similar properties, as the kernel weights are learned throughout the training process.
%         \end{itemize}
%     \end{itemize}
% \end{vbframe}
% % 
% % \frame{
% % \frametitle{1D Cross-correlation Animation}
% %     \center
% %         \only<1>{\includegraphics[width=11cm]{figure_man/cnn/xcorrel_animations/xcorrel_anim_1.png}}%
% %         \only<2>{\includegraphics[width=11cm]{figure_man/cnn/xcorrel_animations/xcorrel_anim_2.png}}%
% %         \only<3>{\includegraphics[width=11cm]{figure_man/cnn/xcorrel_animations/xcorrel_anim_3.png}}%
% %         \only<4>{\includegraphics[width=11cm]{figure_man/cnn/xcorrel_animations/xcorrel_anim_4.png}}%
% %         \only<5>{\includegraphics[width=11cm]{figure_man/cnn/xcorrel_animations/xcorrel_anim_5.png}}%
% %         \only<6>{\includegraphics[width=11cm]{figure_man/cnn/xcorrel_animations/xcorrel_anim_6.png}}%
% %         \only<7>{\includegraphics[width=11cm]{figure_man/cnn/xcorrel_animations/xcorrel_anim_7.png}}%
% %         \only<8>{\includegraphics[width=11cm]{figure_man/cnn/xcorrel_animations/xcorrel_anim_8.png}}%
% %         \only<9>{\includegraphics[width=11cm]{figure_man/cnn/xcorrel_animations/xcorrel_anim_9.png}}%
% %         \only<10>{\includegraphics[width=11cm]{figure_man/cnn/xcorrel_animations/xcorrel_anim_10.png}}%
% %         \only<11>{\includegraphics[width=11cm]{figure_man/cnn/xcorrel_animations/xcorrel_anim_11.png}}%
% %         \only<12>{\includegraphics[width=11cm]{figure_man/cnn/xcorrel_animations/xcorrel_anim_12.png}}%
% %         \only<13>{\includegraphics[width=11cm]{figure_man/cnn/xcorrel_animations/xcorrel_anim_13.png}}%
% %         \only<14>{\includegraphics[width=11cm]{figure_man/cnn/xcorrel_animations/xcorrel_anim_14.png}}%
% %         \only<15>{\includegraphics[width=11cm]{figure_man/cnn/xcorrel_animations/xcorrel_anim_15.png}}%
% %         \only<16>{\includegraphics[width=11cm]{figure_man/cnn/xcorrel_animations/xcorrel_anim_16.png}}%
% %         \only<17>{\includegraphics[width=11cm]{figure_man/cnn/xcorrel_animations/xcorrel_anim_17.png}}%
% %         \only<18>{\includegraphics[width=11cm]{figure_man/cnn/xcorrel_animations/xcorrel_anim_18.png}}%
% %         \only<19>{\includegraphics[width=11cm]{figure_man/cnn/xcorrel_animations/xcorrel_anim_19.png}}%
% %         \only<20>{\includegraphics[width=11cm]{figure_man/cnn/xcorrel_animations/xcorrel_anim_20.png}}%
% %         \only<21>{\includegraphics[width=11cm]{figure_man/cnn/xcorrel_animations/xcorrel_anim_21.png}}%
% %         \only<22>{\includegraphics[width=11cm]{figure_man/cnn/xcorrel_animations/xcorrel_anim_22.png}}%
% %         \only<23>{\includegraphics[width=11cm]{figure_man/cnn/xcorrel_animations/xcorrel_anim_23.png}}%
% %         \only<24>{\includegraphics[width=11cm]{figure_man/cnn/xcorrel_animations/xcorrel_anim_24.png}}%
% %         \only<25>{\includegraphics[width=11cm]{figure_man/cnn/xcorrel_animations/xcorrel_anim_25.png}}%
% %         \only<26>{\includegraphics[width=11cm]{figure_man/cnn/xcorrel_animations/xcorrel_anim_26.png}}%
% %         \only<27>{\includegraphics[width=11cm]{figure_man/cnn/xcorrel_animations/xcorrel_anim_27.png}}%
% %         \only<28>{\includegraphics[width=11cm]{figure_man/cnn/xcorrel_animations/xcorrel_anim_28.png}}%
% %         \only<29>{\includegraphics[width=11cm]{figure_man/cnn/xcorrel_animations/xcorrel_anim_29.png}}%
% % }
% 
% \frame{
% \frametitle{1D Cross-correlation Animation}
%     \center
%         \only<1>{\includegraphics[width=11cm]{figure_man/cnn/xcorrel_animations/xcorrel_anim_3.png}}%
%         \only<2>{\includegraphics[width=11cm]{figure_man/cnn/xcorrel_animations/xcorrel_anim_10.png}}%
%         \only<3>{\includegraphics[width=11cm]{figure_man/cnn/xcorrel_animations/xcorrel_anim_15.png}}%
%         \only<4>{\includegraphics[width=11cm]{figure_man/cnn/xcorrel_animations/xcorrel_anim_19.png}}%
%         \only<5>{\includegraphics[width=11cm]{figure_man/cnn/xcorrel_animations/xcorrel_anim_24.png}}%
%         \only<6>{\includegraphics[width=11cm]{figure_man/cnn/xcorrel_animations/xcorrel_anim_28.png}}%
% }
% 
% 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{vbframe}{Cross-correlation vs. Convolution}
%     \begin{figure}
%         \centering
%         \includegraphics[width=4cm]{figure_man/cnn/other/conv_xcorrel.png}
%         \caption{Comparison of convolution and cross-correlation}
%     \end{figure}
% \end{vbframe}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % \begin{vbframe}{Summary}
% %   \begin{figure}
% %   \centering
% %     \includegraphics[width=11cm]{figure_man/cnn/03_first_glimpse/cnn11.png}
% %     \caption{As we now defined the first part of a convolutional layer, let's move on the pooling op}
% %   \end{figure}
% % \end{vbframe}
% 







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%          REFERENCES          %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}
\frametitle{References}
\footnotesize{
\begin{thebibliography}{99}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Ian Goodfellow et al., 2016]{1} Ian Goodfellow, Yoshua Bengio and Aaron Courville (2016)
\newblock Deep Learning
\newblock \emph{\url{http://www.deeplearningbook.org/}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Otavio Good, 2015]{2} Otavio Good (2015)
\newblock How Google Translate squeezes deep learning onto a phone
\newblock \emph{\url{https://research.googleblog.com/2015/07/how-google-translate-squeezes-deep.html}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Zhang et al., 2016]{3} Zhang, Richard and Isola, Phillip and Efros, Alexei A (2016)
\newblock Colorful Image Colorization
\newblock \emph{\url{https://arxiv.org/pdf/1603.08511.pdf}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Mariusz Bojarski et al., 2016]{4} Mariusz Bojarski, Davide Del Testa,Daniel Dworakowski,Bernhard Firner,Beat Flepp, Prasoon Goyal, Lawrence D. Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, Xin Zhang, Jake Zhao, Karol Zieba (2016)
\newblock End to End Learning for Self-Driving Cars
\newblock \emph{\url{https://arxiv.org/abs/1604.07316}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Namrata Anand and Prateek Verma, 2016]{5} Namrata Anand and Prateek Verma (2016)
\newblock Convolutional and recurrent nets for detecting emotion from audio data
\newblock \emph{\url{http://cs231n.stanford.edu/reports/2015/pdfs/Cs_231n_paper.pdf}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Mnih Volodymyr, 2013]{8} Mnih Volodymyr (2013)
\newblock Machine Learning for Aerial Image Labeling
\newblock \emph{\url{https://www.cs.toronto.edu/~vmnih/docs/Mnih_Volodymyr_PhD_Thesis.pdf}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Hyeonwoo Noh et al., 2013]{9} Hyeonwoo Noh, Seunghoon Hong and Bohyung Han (2015)
\newblock Learning Deconvolution Network for Semantic Segmentation
\newblock \emph{\url{http://arxiv.org/abs/1505.04366}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Karen Simonyan and Andrew Zisserman 2014]{10} Karen Simonyan and Andrew Zisserman (2014)
\newblock Very Deep Convolutional Networks for Large-Scale Image Recognition
\newblock \emph{\url{https://arxiv.org/abs/1409.1556}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Alex Krizhevsky et al., 2012]{11} Alex Krizhevsky, Ilya Sutskever and Geoffrey E. Hinton (2012)
\newblock ImageNet Classification with Deep Convolutional Neural Networks
\newblock \emph{\url{https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf}}


\end{thebibliography}
}
\end{vbframe}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\frame{

\frametitle{Padding}

  \begin{itemize}

    \only<1-3>{\item \enquote{Valid} convolution without padding.}
    \only<1>{\item[] Exactly what we just did is called valid convolution. Suppose we have an input of size $5 \times 5$ and a filter of size $2$. }
    \only<2>{\item[] The filter is only allowed to move inside of the input space.}
    \only<3>{\item[] That will inevitably reduce the output dimensions.}
    
  \end{itemize}

  \center
  \only<1>{\scalebox{0.92}{\includegraphics{figure_man/cnn/05_conv_variations/valid/valid_conv_2.jpg}}}%
  \only<2>{\scalebox{1.05}{\includegraphics{figure_man/cnn/05_conv_variations/valid/valid_conv_3.jpg}}}%
  %\only<3>{\includegraphics[width=10cm]{figure_man/cnn/05_conv_variations/valid/valid2.png}}%
  %\only<4>{\includegraphics[width=10cm]{figure_man/cnn/05_conv_variations/valid/valid3.png}}%
  \only<3>{\scalebox{0.92}{\includegraphics{figure_man/cnn/05_conv_variations/valid/valid_conv_1n.png}}}%
  
  \only<3>{\item[] In general, for an input of size $i \:(\times \:i)$ and filter size $k \:(\times \:k)$, the size $o \:(\times \:o)$ of the output feature map is:
    $$ o=  i-k + 1 $$ }
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{

\frametitle{Padding}

  \begin{itemize}

    \only<1-4>{\item Convolution with \enquote{same} padding.}
    \only<1>{\item[] Suppose the following situation: an input with dimensions $5x5$ and a filter with size $3$.}
    \only<2>{\item[] We would like to obtain an output with the same dimensions as the input.}
    \only<3>{\item[] Hence, we apply a technique called zero padding. That is to say \enquote{pad} zeros around the input:}
    \only<4>{\item[] That always works! We just have to adjust the zeros according to the input dimensions and filter size (ie. one, two or more rows).}

  \end{itemize}

  \center
  \only<1>{\includegraphics[width=11cm]{figure_man/cnn/05_conv_variations/same/same0.png}}%
  \only<2>{\includegraphics[width=11cm]{figure_man/cnn/05_conv_variations/same/same1.png}}%
  \only<3>{\includegraphics[width=11cm]{figure_man/cnn/05_conv_variations/same/same2.png}}%
  \only<4>{\includegraphics[width=11cm]{figure_man/cnn/05_conv_variations/same/same7.png}}%

}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\frame{

\frametitle{Padding and Network Depth}

\begin{figure}
\center
  \scalebox{0.58}{\includegraphics{figure_man/cnn/zeropadding.png}}
\caption{\small{\enquote{Valid} versus \enquote{same} convolution. \emph{Top} : Without padding, the width of the feature map shrinks rapidly to 1 after just three convolutional layers (filter width of 6 shown in each layer). This limits how deep the network can be made. {Bottom} : With zero padding (shown as solid circles), the feature map can remain the same size after each convolution which means the network can be made arbitrarily deep. (Goodfellow, \emph{et al.}, 2016, ch.~9)}}
\end{figure}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Strides}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\frame{

\frametitle{Strides}

  \begin{itemize}

    \only<1-3>{\item Stepsize \enquote{strides} of our filter (stride = 2 shown below).}

  \end{itemize}

  \center
  \only<1>{\scalebox{0.7}{\includegraphics{figure_man/cnn/05_conv_variations/strides/strides0.png}}}%
  \only<2>{\scalebox{0.7}{\includegraphics{figure_man/cnn/05_conv_variations/strides/strides1.png}}}%
  %\only<3>{\includegraphics[width=9cm]{figure_man/cnn/05_conv_variations/strides/strides2.png}}%
  %\only<4>{\includegraphics[width=9cm]{figure_man/cnn/05_conv_variations/strides/strides3.png}}%
  \only<3>{\scalebox{0.7}{\includegraphics[width=9cm]{figure_man/cnn/05_conv_variations/strides/strides4.png}}}%
  
  \only<3>{\item[] In general, when there is no padding, for an input of size $i$, filter size $k$ and stride $s$, the size $o$ of the output feature map is:
    $$ o=\left\lfloor\frac{i-k}{s}\right\rfloor+ 1 $$ }

}

\frame{

\frametitle{Strides and downsampling}

\begin{figure}
\center
\includegraphics[width=.5\textwidth]{figure_man/cnn/stride.png}
\caption{A strided convolution is equivalent to a convolution without strides followed by downsampling (Goodfellow, \emph{et al.}, 2016, ch.~9).}
\end{figure}


}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





\section{Max Pooling}

\frame{
\frametitle{Max Pooling}

  
  
  \center
  \only<1>{\includegraphics[width=10cm]{figure_man/cnn/08_pooling/pool1.png}}%
  \only<2>{\includegraphics[width=10cm]{figure_man/cnn/08_pooling/pool3.png}}%
  \only<3>{\includegraphics[width=10cm]{figure_man/cnn/08_pooling/pool6.png}}%
  %\only<4>{\includegraphics[width=10cm]{figure_man/cnn/08_pooling/pool8.png}}%
  
  \begin{itemize}
    \only<1>{\item We've seen how convolutions work, but there is one other operation we need to understand.}
    \only<1>{\item We want to downsample the feature map but optimally lose no information.}
    \only<2>{\item Applying the max pooling operation, we simply look for the maximum value at each spatial location.}
    \only<2>{\item That is 8 for the first location.}
    \only<2>{\item  Due to the filter of size 2 we halve the dimensions of the original feature map and obtain downsampling.}
    \only<3>{\item The final pooled feature map has entries 8, 6, 9 and 3.
    \item Popular pooling functions: max and (weighted) average.}
    %\only<4>{\item Blurring the image by randomly changing pixel entries by either +1 or -1 will only marginally change activations.}
    %\only<4>{\item Next to the dimension and thus parameter reduction, pooling can increase the robustness of the net.}
  \end{itemize}
}



\frame{

\frametitle{Invariance to small translation}

\begin{figure}
\center
\scalebox{0.51}{\includegraphics{figure_man/cnn/maxpooling.png}}
\caption{\footnotesize{Max pooling introduces invariance to small translations of the input. (\emph{Top}) A view of the middle of the output of a convolutional layer. The bottom row shows outputs of the nonlinearity. The top row shows the outputs of max pooling, with a stride of \textbf{one} pixel between pooling regions and a pooling region width of three pixels. (\emph{Bottom}) A view of the same network, after the input has been shifted to the right by one pixel. Every value in the bottom row has changed, but only half of the values in the top row have changed, because the max pooling units are sensitive only to the maximum value in the neighborhood, not its exact location (Goodfellow, \emph{et al.}, 2016, ch.~9).}}
\end{figure}


}

\begin{vbframe}{Convolution and Pooling as infinitely strong prior}
\begin{itemize}
\item Recall the Bayesian perspective of probability:
\begin{equation*}
\underbrace{P ( \theta | \mathcal{D} )}_{\text{posterior}} \sim  \underbrace{P ( \mathcal{D} | \theta )}_{\text{data likelihood}} \underbrace{P ( \theta )}_{\text{prior}}  \,.
\end{equation*}
\item Convolution and pooling can be seen as infinitely strong prior
over the parameters of a fully connected net.
\item Infinitely strong means that the prior places 0 probability on some parameters despite the support by data. 
\item The prior introduced by convolutions says that the function the layer should learn contains only local interactions and is equivariant to translation. 
\item The prior introduced by pooling says that each unit should be invariant to small translations.
\framebreak
\item Two key insights from this view are:
\begin{enumerate}
\item Convolution and pooling can cause underfitting. For example, using pooling on all features can increase the training error, if a task relies on preserving precise spatial information. Furthermore, using convolution if a task relies on incorporating information from very distant locations may be inappropriate.
\item Convolutional models should only be compared to other convolutional models in benchmarks. Models that do not use convolution would be able to learn even if we permuted all the pixels in the image.
\end{enumerate}
\end{itemize}
\end{vbframe}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%          REFERENCES          %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}
\frametitle{References}
\footnotesize{
\begin{thebibliography}{99}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Ian Goodfellow et al., 2016]{1} Ian Goodfellow, Yoshua Bengio and Aaron Courville (2016)
\newblock Deep Learning
\newblock \emph{\url{http://www.deeplearningbook.org/}}


\end{thebibliography}
}
\end{vbframe}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \section{Different Perspectives of CNNs}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{vbframe}{CNNs - Perspective I}
\center
\includegraphics[width=9cm]{figure_man/cnn/03_first_glimpse/cnn_scheme.png}
\begin{itemize}
\item Schematic architecture of a CNN.
\item The input tensor is convolved by different filters yielding different feature maps (coloured) in subsequent layers.
\item A dense layer connects the final feature maps with the softmax-activated output neurons.
\end{itemize}
\end{vbframe}

\begin{vbframe}{CNNs - Perspective II}
\center
\includegraphics[width=11cm]{figure_man/cnn/03_first_glimpse/cnn_flat.png}
\begin{itemize}
\item Flat view of a CNN architecture for a classification problem.
\item Consists of 2 CNN layers that are each followed by max-pooling, then flattened and connected with the final output neurons via a dense layer.
\end{itemize}
\end{vbframe}

\begin{vbframe}{CNNs - Perspective III}
\center
\includegraphics[width=8cm]{figure_man/cnn/03_first_glimpse/3dviz_fcn.png}
\begin{itemize}
% \item Awesome interactive visualization by \cite{29} (\href{http://scs.ryerson.ca/~aharley/vis/}{click here})
\item Awesome interactive visualization (by Adam Harley)  \href{http://scs.ryerson.ca/~aharley/vis/}{\beamergotobutton{Click here}}.
\item Vanilla 2-layer densely-connected net on MNIST data for input digit $3$.
\item Each neuron in layer 1 is connected to each of the input neurons.
\end{itemize}
\framebreak
\center
\includegraphics[width=8cm]{figure_man/cnn/03_first_glimpse/3dviz_cnn_front.png}
\begin{itemize}
\item Front view on 2-layer CNN with Pooling and final dense layer on MNIST data for input digit $3$.
\item Each neuron in the second CNN layer is connected to a patch of neurons from each of the previous feature maps via the convolutional kernel.
\end{itemize}
\framebreak
\center
\includegraphics[width=7.5cm]{figure_man/cnn/03_first_glimpse/3dviz_cnn_bottom.png}
\begin{itemize}
\item Bottom view on 2-layer CNN with Pooling and final dense layer on MNIST data for input digit $3$.
\item Each neuron in the second CNN layer is connected to a patch of neurons from each of the previous feature maps via the convolutional kernel.
\end{itemize}
\end{vbframe}



\frame{
    \frametitle{CNNs - Architecture}
    
    \center
    %\only<1>{\includegraphics[width=11cm]{figure_man/cnn/03_first_glimpse/cnn_scheme.png}}%
        \only<1>{\includegraphics[width=11cm]{figure_man/cnn/03_first_glimpse/cnn1}}%
    \only<2>{\includegraphics[width=11cm]{figure_man/cnn/03_first_glimpse/cnn2}}%
    \only<3>{\includegraphics[width=11cm]{figure_man/cnn/03_first_glimpse/cnn3}}%
    \only<4>{\includegraphics[width=11cm]{figure_man/cnn/03_first_glimpse/cnn4}}%
    \only<5>{\includegraphics[width=11cm]{figure_man/cnn/03_first_glimpse/cnn5}}%
    \only<6>{\includegraphics[width=11cm]{figure_man/cnn/03_first_glimpse/cnn6}}%
    \only<7>{\includegraphics[width=11cm]{figure_man/cnn/03_first_glimpse/cnn7}}%
    \only<8>{\includegraphics[width=11cm]{figure_man/cnn/03_first_glimpse/cnn8}}%
    \only<9>{\includegraphics[width=11cm]{figure_man/cnn/03_first_glimpse/cnn9}}%
    \only<10>{\includegraphics[width=11cm]{figure_man/cnn/03_first_glimpse/cnn9}}%
    \only<11>{\includegraphics[width=11cm]{figure_man/cnn/03_first_glimpse/cnn11}}%
    \only<12>{\includegraphics[width=11cm]{figure_man/cnn/03_first_glimpse/cnn11}}%
    \only<13>{\includegraphics[width=11cm]{figure_man/cnn/03_first_glimpse/cnn12}}%
    
    \begin{itemize}
    %\only<1>{\item Schematic architecture of a CNN}
    \only<1>{\item Suppose we have the following input tensor with dimensions $10 \times 10 \times 3$.}
    \only<2>{\item We use a filter of size $2$.}
    \only<3>{\item Applying it to the first spatial location, yields one scalar value.}
    \only<4>{\item The second spatial location yields another one..}
    \only<5>{\item ...and another one...}
    \only<6>{\item ...and another one...}
    \only<7>{\item Finally we obtain an output which is called feature map.}
    \only<8>{\item We initialize another filter to obtain a second feature map.}
    \only<9>{\item All feature maps yield us a \enquote{new image} with dim $h \times w \times N$.}
    \only<10>{\item We actually append them to a new tensor with depth = \# filters.}
    \only<11>{\item All feature map entries will then be activated (e.g. via ReLU), just like the neurons of a standard feedforward net.}
    \only<12>{\item One may use pooling operations to downsample the dimensions of the feature maps.}
    \only<12>{\item Pooling is applied on each feature map independently: the latter, blue block is the pooled version of the previous, blue feature map.}
    \only<13>{\item Many of these layers can be placed successively, to extract evermore complex features.}
    \only<13>{\item The feature maps are fed into each other sequentially. For instance, each filter from the second conv layer gets all previous feature maps from the first conv layer as an input. Each filter from the first layer extracts information from the input image tensor.}
    \only<13>{\item The feature maps of the final conv layer are flattened (into a vector) and fed into a dense layer which, in turn, is followed by more dense layers and finally, the output layer.}
    \end{itemize}
    }
    
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        
        
        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        
        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        % BACKPROP NOT TREATED IN WS2018
    % TO BE DONE MORE CONCRETELY AND WITH MATH. DEPTH 
    % e.g.: https://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/
        
        % \begin{vbframe}{Backpropagation}
    %   \begin{figure}
    %   \centering
    %     \includegraphics[width=7cm]{figure_man/cnn/06_conv_properties/ps/dense1.png}
    %   \end{figure}
    %   \begin{itemize}
    %     \item Assume a dense net. We had to compute 36 different gradients to adjust our network (36 weights).
    %   \end{itemize}
    % \end{vbframe}
    % 
    % \frame{
        % \frametitle{Backpropagation}
        % 
        %   \center
        %   \only<1>{\includegraphics[width=6.5cm]{figure_man/cnn/09_backprop/bp1.png}}%
            %   \only<2>{\includegraphics[width=6.5cm]{figure_man/cnn/09_backprop/bp2.png}}%
            %   \only<3>{\includegraphics[width=6.5cm]{figure_man/cnn/09_backprop/bp4.png}}%
            % 
        %   \begin{itemize}
        % 
        %     \only<1>{\item We've already learned that our CNN only has 4 weights.}
%     \only<2>{\item Let us focus once again on weight $w_{11}$}
%     \only<3>{\item The highlited connections shows where $w_{11}$ incorporates.}
% 
%   \end{itemize}
% }
% 
% \frame{
% \frametitle{Backpropagation}
% 
%   \begin{itemize}
% 
%     \only<1-5>{\item We know from earlier computations:}
%     \only<1-2>{\item[] $s_{11} = a \cdot w_{11} + b \cdot w_{12} + d \cdot w_{21} + e \cdot w_{22}$}
%     \only<1-2>{\item[] $s_{12} = b \cdot w_{11} + c \cdot w_{12} + e \cdot w_{21} + f \cdot w_{22}$}
%     \only<1-2>{\item[] $s_{21} = d \cdot w_{11} + e \cdot w_{12} + g \cdot w_{21} + h \cdot w_{22}$}
%     \only<1-2>{\item[] $s_{22} = e \cdot w_{11} + f \cdot w_{12} + h \cdot w_{21} + i \cdot w_{22}$}
%     \only<3-4>{\item[] $s_{11} = a \cdot \textcolor{red}{w_{11}} + b \cdot w_{12} + d \cdot w_{21} + e \cdot w_{22}$}
%     \only<3-4>{\item[] $s_{12} = b \cdot \textcolor{red}{w_{11}} + c \cdot w_{12} + e \cdot w_{21} + f \cdot w_{22}$}
%     \only<3-4>{\item[] $s_{21} = d \cdot \textcolor{red}{w_{11}} + e \cdot w_{12} + g \cdot w_{21} + h \cdot w_{22}$}
%     \only<3-4>{\item[] $s_{22} = e \cdot \textcolor{red}{w_{11}} + f \cdot w_{12} + h \cdot w_{21} + i \cdot w_{22}$}
%     \only<5>{\item[] $s_{11} = \textcolor{red}{a} \cdot w_{11} + b \cdot w_{12} + d \cdot w_{21} + e \cdot w_{22}$}
%     \only<5>{\item[] $s_{12} = \textcolor{red}{b} \cdot w_{11} + c \cdot w_{12} + e \cdot w_{21} + f \cdot w_{22}$}
%     \only<5>{\item[] $s_{21} = \textcolor{red}{d} \cdot w_{11} + e \cdot w_{12} + g \cdot w_{21} + h \cdot w_{22}$}
%     \only<5>{\item[] $s_{22} = \textcolor{red}{e} \cdot w_{11} + f \cdot w_{12} + h \cdot w_{21} + i \cdot w_{22}$}
%     \only<1-5>{\item[]}
%     \only<2-5>{\item To obtain gradients for our weights, we simply compute:}
%     \only<1-5>{\item[]}
%     \only<2>{\item
%       $\frac{\delta s_{i,j}}{\delta w}$ = $\begin{pmatrix} 
%       \sum\limits_{i} \sum\limits_{j} \frac{\delta s_{ij}}{\delta w_{11}} & 
%       \sum\limits_{i} \sum\limits_{j} \frac{\delta s_{ij}}{\delta w_{12}} \\ 
%       \sum\limits_{i} \sum\limits_{j} \frac{\delta s_{ij}}{\delta w_{21}} & 
%       \sum\limits_{i} \sum\limits_{j} \frac{\delta s_{ij}}{\delta w_{22}}
%       \end{pmatrix}$    
%     }
%     \only<3>{\item
%       $\frac{\delta s_{i,j}}{\delta w}$ = $\begin{pmatrix} 
%       \sum\limits_{i} \sum\limits_{j} \frac{\delta s_{ij}}{\delta \textcolor{red}{w_{11}}} & 
%       \sum\limits_{i} \sum\limits_{j} \frac{\delta s_{ij}}{\delta w_{12}} \\ 
%       \sum\limits_{i} \sum\limits_{j} \frac{\delta s_{ij}}{\delta w_{21}} & 
%       \sum\limits_{i} \sum\limits_{j} \frac{\delta s_{ij}}{\delta w_{22}}
%       \end{pmatrix}$    
%     }
%     \only<4>{\item 
%       $\frac{\delta s_{i,j}}{\delta w}$ = $\begin{pmatrix} 
%       \frac{\delta s_{11}}{\delta \textcolor{red}{w_{11}}} + 
%       \frac{\delta s_{12}}{\delta \textcolor{red}{w_{11}}} + 
%       \frac{\delta s_{21}}{\delta \textcolor{red}{w_{11}}} + 
%       \frac{\delta s_{22}}{\delta \textcolor{red}{w_{11}}} & &
%       .... \\
%       \textcolor{white}{bla} \\
%       .... & &
%       ....
%       \end{pmatrix}$
%     }
%     \only<5>{\item
%       $\frac{\delta s_{i,j}}{\delta w}$ = $\begin{pmatrix} 
%       \textcolor{red}{a} + \textcolor{red}{b} + \textcolor{red}{d} + \textcolor{red}{e} & &
%       b + c + e + f \\
%       \textcolor{white}{bla} \\
%       d + e + g + h & &
%       e + f + h + i
%       \end{pmatrix}$    
%     }
%   \end{itemize}
% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{vbframe}{Convolution mathematically - content}
%     \begin{itemize}
%         \item Sources:
%         \item nice explanation here: https://wiki.tum.de/display/lfdv/Convolutional+Neural+Networks
%         \item and here: https://wiki.tum.de/display/lfdv/Layers+of+a+Convolutional+Neural+Network
%         \item and dl book: http://www.deeplearningbook.org/contents/convnets.html
%         \item cornell uni: google cornell cs1114 convolution
%     \end{itemize}
% \end{vbframe}    


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Applications}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Application - Image Classification}
    \begin{itemize}
        \item The standard use case for CNNs is image classification.
        \item There exist a broad variety of battle-proven image classification architecture such as the AlexNet , the Inception Net or the ResNet  which will be discussed in detail in the next lecture.
        \item All those architectures rely on a set of subsequent convolutional filters and aim to learn the mapping from an image to a probability score over a set of classes.
    \end{itemize}
\framebreak
    \begin{figure}
        \centering
        \includegraphics[width=7cm]{figure_man/cnn/01_introduction/recognition.png}
        \caption{Image classification with Cifar 10: famous benchmark dataset with 60000 images and 10 classes (Alex Krizhevsky (2009)). There is also a much more difficult version with 60000 images and 100 classes.}
    \end{figure}
\framebreak
    \begin{figure}
        \centering
        \includegraphics[width=4cm]{figure_man/cnn/application/cifar_frog.png}
        \caption{One example of the Cifar10 data: A highly pixelated, coloured image of a frog with dimension [32, 32, 3]. }
    \end{figure}
\framebreak
    \begin{figure}
        \centering
          \scalebox{1}{\includegraphics{figure_man/cnn/application/cifar10_eg.png}}
        \caption{An example of a CNN architecture for classification on the Cifar10 dataset (FC = Fully Connected). }
    \end{figure}  
\end{vbframe}

\begin{frame} {CNN vs a Fully Connected net on Cifar10}
  \begin{figure}
        \centering
          \scalebox{0.75}{\includegraphics{figure_man/cnn/application/cnn_vs_dense_1.png}}
        \caption{Performance of a CNN and a fully connected neural net ("Dense") on Cifar-10. Both networks have roughly the same number of layers and learnable parameters ($\sim$ 800k). They were trained using the same learning rate, weight decay and dropout rate. Of course, the CNN performs better because it has the right inductive bias for the task.}
    \end{figure} 
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Application - Image Colorization}
    \begin{itemize}
        \item Basic idea (introduced by Zhang et al., 2016):
        \begin{itemize}
            \item Train the net on pairs of grayscale and coloured images.
            \item Force it to make a prediction on the colour-value \textbf{for each pixel} in the grayscale input image.
            \item Combine the grayscale-input with the colour-output to yield a colorized image.
        \end{itemize}
        \item Very comprehensive material on the method is provided on the author's website. \href{http://richzhang.github.io/colorization/}{\beamergotobutton{Click here}}
            \end{itemize}
            \framebreak
            \begin{figure}
            \centering
            \includegraphics[width=11cm]{figure_man/cnn/application/fish_lab.png}
            \caption{The CNN learns the mapping from grayscale (L) to color (ab) for each pixel in the image. The L and ab maps are then concatenated to yield the colorized image. The authors use the LAB color space for the image representation.}
            \end{figure}
            \framebreak
            \begin{figure}
            \centering
            \includegraphics[width=5cm]{figure_man/cnn/application/lab.png}
            \caption{The colour space (ab) is quantized in a total of 313 bins. This allows to treat the color prediction as a classification problem where each pixel is assigned a probability distribution over the 313 bins and that with the highest softmax-score is taken as predicted color value. The bin is then mapped back to the corresponding, numeric (a,b) values. The network is optimized using a multinomial cross entropy loss over the 313 quantized (a,b) bins.}
            \end{figure}
            \framebreak
            \begin{figure}
            \centering
            \includegraphics[width=11.5cm]{figure_man/cnn/application/color_architecture.png}
            \caption{The architecture consists of stacked CNN layers which are upsampled towards the end of the net. It makes use of \textit{dilated convolutions} and \textit{upsampling layers} which are explained in the next lecture. The output is a tensor of dimension [64, 64, 313] that stores the 313 probabilities for each element of the final, downsampled 64x64 feature maps.} 
            \end{figure}
            \framebreak
            \begin{figure}
            \centering
            \includegraphics[width=11.5cm]{figure_man/cnn/application/color_architecture.png}
            \caption{This block is then upsampled to a dimension of 224x224 and the predicted color bins are mapped back to the (a,b) values yielding a depth of 2. Finally, the L and the ab maps are concatenated to yield a colored image.} 
            \end{figure}
            \end{vbframe}
            %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
                %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
                \begin{vbframe}{Application - Object localization}
            \begin{itemize}
            \item Until now, we used CNNs for \textit{single}-class classification of images - \textbf{which object is on the image?}
            \item Now we extend this framework - \textbf{is there an object in the image and if yes, where and which?}
            \end{itemize}
            \begin{figure}
            \centering
            \includegraphics[width=4cm]{figure_man/cnn/application/localize_cat.png}
            \caption{Classify and detect the location of the cat.}
            \end{figure}
            \framebreak
            \begin{itemize}
            % source1:  https://medium.com/machine-learning-bites/deeplearning-series-objection-detection-and-localization-yolo-algorithm-r-cnn-71d4dfd07d5f
            % source2: https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/object_localization_and_detection.html
            \item Bounding boxes can be defined by the location of the left lower corner as well as the height and width of the box: [$b_x$, $b_y$, $b_h$, $b_w$].
            \item We now combine three tasks (detection, classification and localization) in one architecture.
            \item This can be done by adjusting the label output of the net.
            \item Imagine a task with three classes (cat, car, frog).
            \item In standard classification we would have: 
                $$
                \text{label vector}
            \begin{bmatrix}
            c_{cat}\\
            c_{car} \\
            c_{frog}
            \end{bmatrix}
            \text{and softmax output}
            \begin{bmatrix}
            P(y = cat| X, \theta)\\
            P(y = car| X, \theta)\\
            P(y = frog| X, \theta)
            \end{bmatrix}
            $$
                \end{itemize}
            \framebreak
            \begin{itemize}
            \item We include the information, if there is a object as well as the bounding box parametrization in the label vector.
            \item This gives us the following label vector:
                $$
                \begin{bmatrix}
            b_x\\
            b_y \\
            b_h \\
            b_w \\
            c_o \\
            c_{cat} \\
            c_{car} \\
            c_{frog} \\
            \end{bmatrix} =             
                \begin{bmatrix}
            \text{x coordinate box}\\
            \text{y coordinate box} \\
            \text{height box} \\
            \text{width box} \\
            \text{presence of object, binary} \\
            \text{class cat, one-hot} \\
            \text{class car, one-hot}\\
            \text{class frog, one-hot} \\
            \end{bmatrix}
            $$
                \end{itemize}
            \framebreak
            \begin{figure}
            \centering
            \includegraphics[width=8cm]{figure_man/cnn/application/naive_localization.png}
            \end{figure}
            \begin{itemize}
            \item Naive approach: use a CNN with two heads, one for the class classification and one for the bounding box regression.
            \item But: What happens, if there are two cats in the image?
                \item Different approaches: "Region-based" CNNs (R-CNN, Fast R-CNN and Faster R-CNN) and "single-shot" CNNs (SSD and YOLO).
            \end{itemize}
            % \framebreak
            %     \begin{itemize}
            %         \item \textbf{Q to reviewers: go on or too much? Would require +5 slides to explain Faster RCNN in a rough manner}
            %         \item plenty of good resources out there:
                %         \item \href{https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/object_localization_and_detection.html}{1}
            %         \item \href{https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4}{2}
            %         \item \href{https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e}{3}
            %         \item \href{http://www.robots.ox.ac.uk/~tvg/publications/talks/fast-rcnn-slides.pdf}{4}
            %     \end{itemize}
            \end{vbframe}
            
            
            
            
            
            
            
            
            
            % \begin{vbframe}{Convolution mathematically - 2D}
            %     \begin{itemize}
            %         \item Interpret images as data matrices
            %         \item Amount of channels determines amount of matrices (RGB: 3, BW: 1)
            %         \item One channel of image $\mathcal{I}$ is represented as a matrix:\\
            %             \begin{equation*}
            %                 \begin{split}
            %                 \mathcal{I}:\Omega \in \R^2 &\mapsto \R_+ \\
            %                 (i, j) \mapsto \mathcal{I}_{i, j}
            %                 \end{split}
            %             \end{equation*}
            %         \item $\mathcal{I}$ contains two axis $\rightarrow$ use 2-dimensional kernel:
                %             \begin{equation*}
            %                 \begin{split}
            %                     S(i, j) &= (\mathcal{I} \star \mathcal{K})(i, j) = \sum_{m} \sum_{n} \mathcal{I}(m, n) \mathcal{K}(i-m, j-n) \\
            %                     \text{where } m, n &:= \text{iterators $\mathcal{I}$} \\
            %                     \text{and } i, j &:= \text{iterators positions of } \mathcal{K} \\
            %                 \end{split}
            %             \end{equation*}
            %     \end{itemize}
            % \end{vbframe}
            
            
            %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
                %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
                % \begin{vbframe}{Convolution mathematically - tum}
            %     \begin{itemize}
            %         \item Convolution: filter one function (image I) with another one (filter K) to yield third function (feature map Y)  
                %         \item This can be formulated as: \\
            %                 \begin{equation*}
            %                     Y = (I \star K)_{r, s} := \sum_{u = -h_1}^{h_1} \sum_{u = -h_2}^{h_2} K_{u, v} * I_{r-u, s-v}
            %                 \end{equation*}
            %         \item with filter matrix K:\\
            %                 \begin{equation*}
            %                     K = 
                %                     \begin{bmatrix}
            %                         K_{-h_1, -h_2} & ... & K_{-h_1, h_2} \\
            %                         ... & K_{0, 0} & ... \\
            %                         K_{h_1, - h_2} & ... & K_{h_1, h_2}
            %                     \end{bmatrix}
            %                 \end{equation*}
            %     \end{itemize}
            % \framebreak
            %     \begin{itemize}
            %         \item Each layer $l$ consists of $m_1$ filters $K^{(l)}_i, ..., K^{(l)}_{m_1}$
                %         \item Thus, output $Y_i^{(l)}$ of layer $l$ contains $m_1^{(l)}$ feature maps of dimension $m_2^{(l)} \times m_3^{(l)}$ 
                %         \item The dimensions $m_2^{(l)} \text{ and } m_3^{(l)}$ can be calculated as $XXX$
                %         \item The $i^\text{th}$ feature map is computed as:
                %             \begin{equation*}
            %                 Y_i^{(l)} = B_i^{(l)} + \sum_{j=1}^{m_1^{l-1}}K_{i, j}^{(l)} \star Y_j^{(l-1)}
            %             \end{equation*}
            %         \item Where $B_i^{(l)}$ corresponds to a bias matrix similar to the bias in FFNs and $K_{i, j}^{(l)}$ is the filter that connects the $j^\text{h}$ with the $i^\text{h}$ feature map in the layer $l$. CHECK!
                %     \end{itemize}
            % \end{vbframe}
            
            
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  %%%%%%%%%%%%%%%%%%          REFERENCES          %%%%%%%%%%%%%%%%%%
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{vbframe}
\frametitle{References}
\footnotesize{
  \begin{thebibliography}{99}
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      \bibitem[Ian Goodfellow et al., 2016]{1} Ian Goodfellow, Yoshua Bengio and Aaron Courville (2016)
  \newblock Deep Learning
  \newblock \emph{\url{http://www.deeplearningbook.org/}}
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      \bibitem[Alex Krizhevsky, 2009]{6} Alex Krizhevsky (2009)
  \newblock Learning Multiple Layers of Features from Tiny Images
  \newblock \emph{\url{https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf}}
      %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      \bibitem[Harley Adam W., 2015]{29} Adam W Harley (2015)
  \newblock An Interactive Node-Link Visualization of Convolutional Neural Networks
  \newblock \emph{\url{http://scs.ryerson.ca/~aharley/vis/}}

  
  \end{thebibliography}
}
\end{vbframe}

\endlecture