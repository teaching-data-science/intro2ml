%This file is a child of preamble.Rnw in the style folder
%if you want to add stuff to the preamble go there to make
%your changes available to all childs

<<setup-child, include = FALSE>>=
library(knitr)
set_parent("../style/preamble.Rnw")
@

<<size = "scriptsize", include=FALSE>>=
source("code/functions.R")
@

\input{../latex-math/basic-math}
\input{../latex-math/basic-ml}
\input{../latex-math/ml-nn}


\lecturechapter{3}{Deep Learning- Multi-class Classification}
\lecture{Deep Learning- Multi-class Classification}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Single Hidden Layer Networks for Multi-Class Classification}

\begin{frame} {Multi-class Classification}
  \vspace{20mm}
  \begin{itemize}
    \item We have only considered regression and binary classification problems so far.
    \vspace{5mm}
    \item How can we get a neural network to perform multiclass classification?
  \end{itemize}
\end{frame}

\begin{frame} {Multi-class Classification}
  \begin{itemize}
    \item The first step is to add additional neurons to the output layer.
    \item Each neuron in the layer will represent a specific class (number of neurons in the output layer = number of classes).
    \begin{figure}
      \centering
      \scalebox{0.75}{\includegraphics[width=10.2cm]{plots/neuralnet_new.png}}
        \caption{\footnotesize Structure of a single hidden layer, feed-forward neural network for g-class classification problems (bias term omitted).}
    \end{figure}
  \end{itemize}
\end{frame}

\begin{frame} {Multi-class Classification}
    \vspace{5mm}
    \begin{blocki}{Notation:}
    \item For $g$-class classification, $g$ output units: $$\mathbf{f} = (f_1, \dots, f_g)$$
    \vspace{4mm}
    \item $m$ hidden neurons $z_1, \dots, z_m$, with
    $$ z_j = \sigma(\Wmat_j^\top \xv), \quad j = 1,\ldots,m. $$
 %   \vspace{4mm}
    \item Compute linear combinations of derived features $z$:
    $$ f_{in,k} = \bm{U}_k^\top \hidz, \quad \hidz=(z_1,\dots, z_m)^\top, \quad k = 1,\ldots,g$$
  \end{blocki}
\end{frame}

\begin{frame} {Multi-class Classification}
  \begin{itemize}
    \item The second step is to apply a softmax activation function to the output layer.
    \vspace{4mm}
    \item This gives us a probability distribution over $g$ different possible classes:
    $$ f_{out,k} = \tau_k(f_{in,k}) = \frac{\exp(f_{in,k})}{\sum_{k'=1}^g\exp(f_{in,k'})}$$
    \vspace{2mm}
    \item This is the same transformation used in softmax regression!
    \vspace{4mm}
    \item Derivative $ \frac{\delta\tau(\mathbf{f}_{in})}{\delta \mathbf{f}_{in}} = \text{diag}(\tau(\mathbf{f}_{in})) - \tau(\mathbf{f}_{in}) \tau(\mathbf{f}_{in})^\top $
    \vspace{4mm}
    \item It is a \enquote{smooth} approximation of the argmax operation,
        so $\tau((1, 1000, 2)^\top) \approx (0, 1, 0)^\top$ (picks out 2nd element!).
  \end{itemize}
\end{frame}

\begin{frame} {Multi-class Classification: Example}
  \small{Forward pass (Hidden: Sigmoid, Output: Softmax).}
  \begin{figure}
    \centering
      \only<1>{\scalebox{0.95}{\includegraphics{plots/softie_one.png}}}
      \only<2>{\scalebox{0.95}{\includegraphics{plots/softie_two.png}}}
      \only<3>{\scalebox{0.95}{\includegraphics{plots/softie_three.png}}}
      \only<4>{\scalebox{0.95}{\includegraphics{plots/softie_four.png}}}
      \only<5>{\scalebox{0.95}{\includegraphics{plots/softie_five_a.png}}}
      \only<6>{\scalebox{0.95}{\includegraphics{plots/softie_five_b.png}}}
      \only<7>{\scalebox{0.95}{\includegraphics{plots/softie_six.png}}}
      \only<8>{\scalebox{0.95}{\includegraphics{plots/softie_seven.png}}}

  \end{figure}
\end{frame}

\begin{frame} {Softmax Loss}
  \begin{itemize}
    \vspace{5mm}
    \item The loss function for a softmax classifier is
    $$L(y, \fx) = - \sum_{k = 1}^g [y = k] \log \left( \frac{\exp(f_{in,k})}{\sum_{k'=1}^g\exp(f_{in,k'})}\right)$$
      where $[y = k] = \begin{cases} 1 & \text{ if } y = k \\
      0 & \text{ otherwise }
      \end{cases}$. 
    \vspace{5mm}
    \item This is equivalent to the cross-entropy loss when the label vector $\bm{y}$ is one-hot coded (e.g. $\mathbf{y} = (0,0,1,0)^\top$). 
    % $$
      % L(y, \fx) = - \sum_{k = 1}^g y_i \log \left( \frac{\exp(f_{in,k})}{\sum_{k'=1}^g\exp(f_{in,k'})}\right)
    % $$
    \item Optimization:  Again, there is no analytic solution. % Therefore, we use gradient descent.
  \end{itemize}
\end{frame}


\begin{vbframe}{Single Hidden Layer Networks: Summary}
  \begin{itemize}
    \item We have seen that neural networks are far more flexible than linear models. Neural networks with a single hidden layer are able to approximate any continuous function.
    \item Yet, in reality, there is no way to make full use of the universal approximation property. The learning algorithm will usually not find the best possible model. At best it finds a locally optimal model. 
    \item The XOR example showed us how neural networks extract features to transform the space and actually learn a kernel (learn a representation).
    \item Neural networks can perfectly fit noisy data. Thus, neural networks are endangered to over-fit. This is particularly true for a model with a huge hidden layer.
    \item Fitting neural networks with sigmoidal activation function is nothing else but fitting many weighted logistic regressions!
  \end{itemize}
\end{vbframe}



\endlecture

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
