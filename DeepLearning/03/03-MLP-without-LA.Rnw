%This file is a child of preamble.Rnw in the style folder
%if you want to add stuff to the preamble go there to make
%your changes available to all childs

<<setup-child, include = FALSE>>=
library(knitr)
set_parent("../style/preamble.Rnw")
@

<<size = "scriptsize", include=FALSE>>=
source("code/functions.R")
@

\input{../latex-math/basic-math}
\input{../latex-math/basic-ml}
\input{../latex-math/ml-nn}


\lecturechapter{3}{Deep Learning- MLP without LA}
\lecture{Deep Learning- MLP without LA}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Multi-Layer Feedforward Neural Networks}

\begin{vbframe}{Feedforward neural networks}
  \begin{itemize}
    \vspace{15mm}
    \item We will now extend the model class once again, such that we allow an arbitrary amount of $l$ (hidden) layers.
    \vspace{5mm}
    \item The general term for this model class is (multi-layer) \textbf{feedforward networks} (inputs are passed through the network from left to right, no feedback-loops are allowed)
  \end{itemize}
\framebreak
 
 
\begin{itemize}
\item We can characterize those models by the following chain structure: $$f(\xv) = \tau \circ \phi \circ \sigma^{(l)} \circ \phi^{(l)} \circ \sigma^{(l-1)} \circ \phi^{(l-1)} \circ \ldots \circ \sigma^{(1)} \circ \phi^{(1)}$$ where $\sigma^{(i)}$ and $\phi^{(i)}$ are the activation function and the weighted sum of hidden layer $i$, respectively. $\tau$ and $\phi$ are the corresponding components of the output layer.

%(W^{(1)T}\xv + \biasb^{(1)})

    \vspace{5mm}
    \item Each hidden layer has: 
      \begin{itemize}
        \vspace{2mm}
        \item an associated weight matrix $\Wmat^{(i)}$, bias $\biasb^{(i)}$ and activations $\hidz^{(i)}$ for $i \in \{ 1 \ldots l\}$
        \vspace{2mm}
        \item $\hidz^{(i)} = \sigma^{(i)}(\phi^{(i)}) = \sigma^{(i)}(\Wmat^{(i)T}\hidz^{(i - 1)} + \biasb^{(i)})$ , where $\hidz^{(0)} = \xv$.
      \end{itemize}
    \vspace{5mm}
    \item Again, without non-linear activations in the hidden layers, the network can only learn linear decision boundaries.
  \end{itemize}
\framebreak
  \lz
  \begin{figure}
    \centering
      \includegraphics[width=10.5cm]{plots/deepneuralnet_new.png}
      \caption{Structure of a deep neural network with $l$ hidden layers (bias terms omitted).}
  \end{figure}

\end{vbframe}  


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame} {Feedforward neural networks: Example}
  \begin{figure}
    \centering
\only<1>{\scalebox{0.95}{\includegraphics{plots/deepnet_one.png}}}
\only<2>{\scalebox{0.95}{\includegraphics{plots/deepnet_two.png}}}
\only<3>{\scalebox{0.95}{\includegraphics{plots/deepnet_three.png}}}
\only<4>{\scalebox{0.95}{\includegraphics{plots/deepnet_four.png}}}
\only<5>{\scalebox{0.95}{\includegraphics{plots/deepnet_five.png}}}
\only<6>{\scalebox{0.95}{\includegraphics{plots/deepnet_six.png}}}
\only<7>{\scalebox{0.95}{\includegraphics{plots/deepnet_seven.png}}}
\only<8>{\scalebox{0.95}{\includegraphics{plots/deepnet_eight.png}}}
\only<9>{\scalebox{0.95}{\includegraphics{plots/deepnet_nine.png}}}
\only<10>{\scalebox{0.95}{\includegraphics{plots/deepnet_ten.png}}}
\only<11>{\scalebox{0.95}{\includegraphics{plots/deepnet_eleven.png}}}
\only<12>{\scalebox{0.95}{\includegraphics{plots/deepnet_twelve.png}}}
  \end{figure}
\end{frame}


\endlecture

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
