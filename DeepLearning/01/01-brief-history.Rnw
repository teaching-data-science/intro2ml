<<setup-child, include = FALSE>>=
library(knitr)
set_parent("../style/preamble.Rnw")
@

<<size = "scriptsize", include=FALSE>>=
source("code/functions.R")
@

\input{../latex-math/basic-math}
\input{../latex-math/basic-ml}
\input{../latex-math/ml-nn}

\lecturechapter{1}{Deep Learning- History of Neural Networks}
\lecture{Deep Learning- History of Neural Networks}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{A brief history of neural networks}
\begin{itemize}
\item \pkg{1943:} The first artificial neuron, the "Threshold Logic Unit (TLU)", was proposed by Warren McCulloch \& Walter Pitts.
\begin{figure}
\includegraphics[width=7cm]{plots/mp_neuron.png}
\end{figure}
\begin{itemize}
\item In this model the neuron fires a $+1$ if the input exceeds a certain threshold $\theta$.
\vspace{2mm}
\item However, this model did not have adjustable weights, so learning could only be achieved by changing the threshold $\theta$.
\end{itemize}
\end{itemize}
\framebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  

\begin{itemize}
\item \pkg{1957:} The perceptron was invented by Frank Rosenblatt. 
\begin{figure}
\includegraphics[width=7cm]{plots/perceptron_new.png}
\end{figure}
\begin{itemize}
\item In perceptron, the weights are adjustable and can be learned by learning algorithms. 
\vspace{2mm}
\item The inputs are not restricted to be binary.
\vspace{2mm}
\item Similar to the MP-neuron, the threshold is adjustable, and decision boundaries are linear.
\end{itemize}
\end{itemize}
\framebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{itemize}
\item \pkg{1960:} ADALINE by Bernard Widrow \& Ted Hoff; weights are now adjustable according to the weighted sum of the inputs.
\vspace{.1cm}
\begin{figure}
\includegraphics[width=7cm]{plots/adaline.png}
\end{figure}
\vspace{1cm}
\item \pkg{1965:} Group method of data handling (also known as polynomial neural networks) by Alexey Ivakhnenko. The first learning algorithm for supervised deep feedforward multilayer perceptrons.
\framebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\item \pkg{1969:} The first \enquote{AI Winter} kicked in.
\begin{itemize}
\item Marvin Minsky \& Seymour Papert proved that a perceptron cannot solve the XOR-Problem (linear separability).
\item Less funding $\Rightarrow$ Standstill in AI/DL research
\end{itemize}
\end{itemize}
\begin{figure}
\includegraphics[width=7cm]{plots/orvsxor.png}
\end{figure}
\begin{itemize}
\item \pkg{1985:} Multi-layered perceptron with backpropagation by David Rumelhart, Geoffrey Hinton and Ronald Williams.
\begin{itemize}
\item Efficiently compute derivatives of composite functions.
\item Backpropagation was developed already in 1970 by Linnainmaa.
\end{itemize}
\framebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\item \pkg{1985:} The second \enquote{AI Winter} kicked in.
\begin{itemize}
\item Overly optimistic/exaggerated expectations concerning potential of AI/DL.
\item Angering investors, the phrase \enquote{AI} even reached a pseudoscience status.
\item Kernel machines and graphical models both achieved good results on many important tasks.
\item Some of the fundamental mathematical difficulties in modeling long sequences were identified.
\end{itemize}
\begin{figure}
\includegraphics[width=7cm]{plots/ai_winter.jpg}
\end{figure}
\framebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\item \pkg{2006:} Age of deep neural networks began.
\begin{itemize}
\item Geoffrey Hinton showed that a deep belief network could be efficiently trained using \textit{greedy layer-wise pretraining}.
\item This wave of research popularized the use of the term deep learning to emphasize that researchers were now able to train deeper neural networks than had been possible before.
\item At this time, deep neural networks outperformed competing AI systems based on other ML technologies as well as hand-designed functionality.
\end{itemize}
\begin{figure}
\includegraphics[width=6.5cm]{plots/dl_feature2.png}
\end{figure}
\framebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Why now and not earlier?}
\begin{itemize}
\item Significantly bigger datasets.
\item Better algorithms and the vanishing gradient problem (optimization chapter).
\item Better regularization (regularization chapter).
\item Unsupervised representation learning (autoencoder chapter).
\item More layers inevitably lead to a significant increase of parameters.
\item Back then, processing power was simply not capable to handle such huge amounts of parameters.
$\Rightarrow$ Nowadays, deep neural networks are trained on GPUs (graphic processing units), not on CPUs (central processing units).
\item Investment by industries and universities.
\item Deep learning tools make learning and applying deep learning easier.
\end{itemize}
\end{itemize}
\framebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}
\centering
\scalebox{1.1}{\includegraphics{plots/dl_timeline.png}}
\caption{Credit: Favio Vazquez}
\framebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\scalebox{1.1}{\includegraphics{plots/DL_tools.png}}
\end{figure}
\framebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}
\centering
\includegraphics[width=8cm]{plots/bostondynamic.png}
\caption{Boston Dynamics (\href{https://www.youtube.com/watch?v=fUyU3lKzoio&ab_channel=BostonDynamics}{click here})}
\end{figure}
\footnotesize
\begin{itemize}
\item Boston Dynamics is a world leader in mobile robots founded in 1992 as a spin-off from the Massachusetts Institute of Technology.
\vspace{.1cm}
\item The company is best known for the development of a series of dynamic highly-mobile robots, including BigDog, Spot, Atlas, and Handle.
\end{itemize}
\framebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}
\centering
\includegraphics[width=8cm]{plots/ibmsupercomputer.jpg}
\caption{IBM Supercomputer}
\end{figure}
\footnotesize
\begin{itemize}
\item Watson is a question-answering system capable of answering questions posed in natural language, developed in IBM's DeepQA project.
\vspace{.1cm}
\item In 2011, Watson competed on \textit{Jeopardy!} against champions Brad Rutter and Ken Jennings, winning the first place prize of $\$ 1$ million.
\end{itemize}
\framebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}
\centering
\includegraphics[width=8cm]{plots/selfdriving.jpg}
\caption{Google self driving car (Waymo)}
\end{figure}
\footnotesize
\begin{itemize}
\item Google's development of self-driving technology began on January 17, 2009, at the company's secretive X lab.
\vspace{.1cm}
\item By January 2020, $20$ million miles of self-driving on public roads had been completed by Waymo.
\end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\endlecture