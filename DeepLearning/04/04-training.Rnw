<<setup-child, include = FALSE>>=
library(knitr)
set_parent("../style/preamble.Rnw")
@

<<size = "scriptsize", include=FALSE>>=
source("code/functions.R")
@

\input{../latex-math/basic-math}
\input{../latex-math/basic-ml}
\input{../latex-math/ml-nn}

\lecturechapter{4}{Deep Learning- Training}
\lecture{Deep Learning- Training}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{Gradient descent}
\begin{itemize}
\item Let $\risk: \R^p \to \R$ be an arbitrary, differentiable, unrestricted function (of $\thetab \in \R^p$). \\
\begin{footnotesize}
In the context of deep learning, $\risk$ represents the empirical risk function and $\thetab$ representes the weights (and biases) of the network. For simplification, we will assume $\bm{\theta} = (\theta_1, ..., \theta_m)$. 
\end{footnotesize}
\item We want to minimize this function by gradient descent (GD). 
\item The negative gradient 
$$- \mathbf{g} = - \nabla \riskt = - \left(\frac{\partial \risk}{\partial \theta_1}, \ldots, \frac{\partial \risk}{\partial \theta_m}\right)^\top$$ 
points in the direction of the \textbf{steepest descent}.
\end{itemize}
\framebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{itemize}
\item \enquote{Standing} at a point $\thetab^{[t]}$ during minimization, we improve by performing the following update: 
$$\thetab^{[t + 1 ]}  = \thetab^{[t]} - \alpha \nabla \risk\left(\thetab^{[t]}\right),$$
which implies (for sufficiently small $\alpha$),
$$\risk(\bm{\thetab}^{[t+1]}) \leq \risk(\bm{\thetab}^{[t]})$$
\item $\alpha$ determines the length of the step and is called \textbf{step size} or, in risk minimization, \textbf{learning rate}.
\end{itemize}
\end{vbframe} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{Example: Gradient descent }
\begin{center}
$\risk(\theta_1, \theta_2) = -\sin(\theta_1) \cdot \frac{1}{2\pi} \exp\left( (\theta_2 - \pi / 2)^2 \right)$
\begin{figure}
\centering
\scalebox{1}{\includegraphics{plots/hill.pdf}}
\end{figure}
\end{center}
\hspace{2cm} "Walking down the hill, towards the valley."\\
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{Gradient Descent and Optimality}
\begin{minipage}{0.45\textwidth}
\begin{small}
\begin{itemize}
\item GD is a greedy algorithm: In every iteration, it makes locally optimal moves.
\vspace*{2mm}
\item If $\riskt$ is \textbf{convex} and \textbf{differentiable}, and its gradient is Lipschitz continuous, GD is guaranteed to converge to the global minimum (for small enough step-size).  
\vspace*{2mm}
\item However, if $\riskt$ has multiple local optima and/or saddle points, GD might only converge to a stationary point (other than the global optimum), depending on the starting point. 
\end{itemize}
\end{small}
\end{minipage}\hfill
\begin{minipage}{0.5\textwidth}
\begin{figure}
\centering
\scalebox{1}{\includegraphics{plots/gdes3.png}}
\end{figure}
\end{minipage}  
\framebreak 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Note: } It might not be that bad if we do not find the global optimum:  
\begin{itemize}
\item We do not optimize the actual quantity of interest, i.e. the (theoretical) risk, but only an approximate version, i.e. the empirical risk. 
\item If the model class is very flexible, it might be disadvantageous to optimize too aggressively and increase the risk of overfitting. 
\item Early-stopping the optimization might even increase generalization performance. 
\end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{Learning rate}
The step-size $\alpha$ plays a key role in the convergence of the algorithm.
\lz
If the step size is too small, the training process may converge \textbf{very} slowly (see left image). If the step size is too large, the process may not converge, because it \textbf{jumps} around the optimal point (see right image).
\begin{center}
\includegraphics[width = 0.3\textwidth]{plots/stepsize_small.png}~~
\includegraphics[width = 0.3\textwidth]{plots/stepsize_large.png}
\end{center}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{Learning Rate}
So far we have assumed a fixed value of $\alpha$ in every iteration:
$$\alpha^{[t]} = \alpha \quad \forall t = {\{1, \ldots, T\}}$$
However, it makes sense to adapt $\alpha$ in every iteration:
\begin{center}
\includegraphics[width = 0.3\textwidth]{plots/stepsize_small.png} ~~~ \includegraphics[width = 0.3\textwidth]{plots/stepsize_adaptive.png} \\
\begin{footnotesize}
Steps of gradient descent for $\riskt = 10\,\theta_1^2 + 0.5\,\theta_2^2$. Left:  100 steps for with a fixed learning rate. Right:  40 steps with an adaptive learning rate.
\end{footnotesize}
\end{center}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{Stochastic gradient descent}
Let us consider GD for empirical risk minimization. The updates are: 
$$\thetab^{[t + 1]} = \thetab^{[t]} - \alpha \cdot \frac{1}{n} \cdot \sumin \nabla_\theta L\left(\yi, f(\xi ~|~ \thetab^{[t]})\right)$$
\begin{itemize}
\item Optimization algorithms that use the entire training set to compute updates in one huge step are called \textbf{batch} or \textbf{deterministic}. This is computationally very costly or often impossible. 
\item \textbf{Idea:} Instead of letting the sum run over the whole dataset (\textbf{batch mode}) one can also let it run only over small subsets (\textbf{minibatches}), or only over a single example $i$. 
\item If the index $i$ of the training example is a random variable with uniform distribution, then its expectation is the batch gradient $\nabla_\theta \risket$
\item[$\to$] We have a \textbf{stochastic}, noisy version of the batch gradient
\framebreak 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\item The gradient w.r.t. a single training observation is fast to compute but not reliable. It can be used simply as a computational trick to deal with large data or to operate on real streams of online data in online learning.
\item In contrast, the full batch gradient is costly (or even impossible, e.g., when data does not even fit into memory) to compute, particularly in DL, but it averages out all the noise from sub-sampling.
\item Minibatches are in between. The batch size decides upon the compromise between speed and averaging (smoothing).
\item In summary: SGD computes an unbiased estimate of the gradient by taking the average gradient over a minibatch (or one sample) to update the parameter $\thetab$ in this direction.
\end{itemize}
\framebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
An illustration of the SGD algorithm (to minimize the function $1.25(x_1 + 6)^2 + (x_2 - 8)^2)$.
\begin{figure}
\scalebox{0.8}{\includegraphics{plots/SGD.png}}
\tiny{\\ source : Shalev-Shwartz and  Ben-David.
Understanding machine learning: From theory to algorithms. Cambridge University Press, 2014. }
\caption{On the left is GD and on the right is SGD. The black line depicts the averaged value of $\thetab$.}
\end{figure}
\framebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{algorithm}[H]
\footnotesize
\caption{Basic SGD pseudo code}
\begin{algorithmic}[1]
\State Initialize parameter vector $\thetab^{[0]}$ 
  
\State $t \leftarrow 0$
\While{stopping criterion not met}
\State Randomly shuffle data and partition into minibatches $J_1, ..., J_K$ of size $m$
\For{$k\in\{1,...,K\}$} 
\State $t \leftarrow t + 1$ 
\State Compute gradient estimate with $J_k$: $\hat{g}^{[t]} \leftarrow \frac{1}{m} \sum_{i \in J_k} \nabla_\theta L(\yi, f(\xi ~|~ \thetab^{[t-1]})) $
\State Apply update: $\thetab^{[t]} \leftarrow \thetab^{[t-1]} - \alpha \hat{g}^{[t]}$
\EndFor
\EndWhile
\end{algorithmic}
\end{algorithm}
\framebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\vspace*{0.5cm}
\begin{itemize}
\item With minibatches of size $m$, a full pass over the training set (called an \textbf{epoch}) consists of $\frac{n}{m}$ gradient updates.
\item SGD and its modifications are the most used optimization algorithms for ML in general and for deep learning in particular.
\item SGD (with one or a few samples per batch) updates have a high variance, even though they are unbiased. Because of this variance, the learning rate $\alpha$ is typically much smaller than in the full-batch scenario.
\framebreak 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\vspace*{0.5cm}
\item When the learning rate is slowly decreased, SGD converges to a local minimum.
\item SGD with minibatches reduces the variance of the parameter updates and utilizes highly optimized matrix operations to efficiently compute gradients.
\item Minibatch sizes are typically between 50 and 1000.
\item Recent results indicate, that SGD often leads to better generalizing models then GD, and thus may perform some kind of indirect regularization.
\end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\endlecture