## Momentum
```{r, include=FALSE, cache=FALSE}
#library(mxnet)
ap = adjust_path(paste0(getwd(), "/figure"))

source("code/functions.R")
```


- While SGD remains a popular optimization strategy, learning with it can sometimes be slow.

- Momentum is designed to accelerate learning, especially when facing high curvature,
  small but consistent or noisy gradients:

  \begin{eqnarray*}
    \nu &\leftarrow& \varphi \nu - \alpha g(\theta) \\
    \theta &\leftarrow& \theta + \nu
  \end{eqnarray*}

- Accumulate an exponentially decaying moving average of past gradients.


<!--
## Momentum

- Exponentially decaying moving average of past gradients:
\vspace{-0.7cm}
\begin{eqnarray*}
  \nu_{1} &\leftarrow& \varphi \nu_0 - \alpha g(\theta_0) \\
  \nu_{2} &\leftarrow& \varphi \nu_1 - \alpha g(\theta_1) \\
          &=& \varphi (\varphi \nu_0 - \alpha g(\theta_0)) - \alpha g(\theta_1) \\
          &=& \varphi^2 \nu_0 - \varphi \alpha g(\theta_0) - \alpha g(\theta_1) \\
  \nu_{3} &\leftarrow& \varphi \nu_2 - \alpha g(\theta_2) \\
          &=& \varphi^3 \nu_0 - \varphi^2 \alpha g(\theta_0) - \varphi \alpha g(\theta_1) - \alpha g(\theta_0) \\
\end{eqnarray*}
\vspace{-0.7cm}
- $\varphi \in [0, 1)$ controls speed of decay, common values are $0.5, 0.9, 0.99$.
  The larger $\varphi$, the more past gradients affect the current direction.

- Momentum depends on how **large** and how **aligned** sequence of gradients is.

## Momentum
-->
<!--
\footnotesize

\begin{eqnarray*}
  \theta_{k+1} &=& \theta_{k} - \alpha \displaystyle\sum_{j = 0}^{k} \varphi^j g(\theta_j) + \varphi^{k+1}\nu_0 \\
               &=& \theta_{k} - \alpha g(\theta) \displaystyle\sum_{j = 0}^{k} \varphi^j + \varphi^{k+1}\nu_0 \\
               &=& \theta_{k} - \alpha g(\theta) \frac{1 - \varphi^{k+1}}{1 - \varphi} + \varphi^k \nu_0 \\
               \lim_{k \to \infty} &=& \theta_k - \alpha g(\theta) \frac{1}{1 - \varphi}
\end{eqnarray*}
-->
<!--

- Suppose momentum always observes the same gradient $g(\theta)$.

- Thus, momentum will accelerate in the direction of $-g(\theta)$ until
reaching terminal velocity with step size:
\[
\nu = -\alpha g(\theta)(1 + \varphi + \varphi^2 + \varphi^3 + ...) = -\alpha g(\theta) \frac{1}{1 - \varphi}
\]

- Hence, $\varphi = 0.9$ corresponds to multiplying the maximum
  speed by 10 relative to the gradient descent algorithm.
-->
<!--

## Momentum: physical explanation

\center
\includegraphics[height = 7.5 cm, width = 7.5 cm]{`r ap("momentum_physics_0.png")`}

## Momentum: physical explanation

\center
\includegraphics[height = 7.5 cm, width = 7.5 cm]{`r ap("momentum_physics_1.png")`}

## Momentum: physical explanation

\center
\includegraphics[height = 7.5 cm, width = 7.5 cm]{`r ap("momentum_physics_2.png")`}

## Momentum: physical explanation

\center
\includegraphics[height = 7.5 cm, width = 7.5 cm]{`r ap("momentum_physics_3.png")`}

## Momentum: physical explanation

\center
\includegraphics[height = 7.5 cm, width = 7.5 cm]{`r ap("momentum_physics_4.png")`}

## Momentum: physical explanation

\center
\includegraphics[height = 7.5 cm, width = 7.5 cm]{`r ap("momentum_physics_5.png")`}


## Gradient descent vs momentum

\begin{figure}
\centering
  \includegraphics[height = 6 cm, width = 10 cm]{`r ap("gd_vs_momentum.png")`}
  \caption{Gradient descent (left) needs many steps and gets stuck
           in a local minimum. Momentum (right) finds the global minimum.}
\end{figure}

-->

## SGD with momentum

\begin{figure}
\centering
  \includegraphics[height = 6 cm, width = 10 cm]{`r ap("momentum.png")`}
  \caption{The contour lines show a quadratic loss function with a poorly
           conditioned Hessian matrix. The to curves show how standard
           gradient descent (black) and momentum (red) learn when dealing
           with ravines.}
\end{figure}



## Momentum in practice

\begin{figure}
\centering
  \includegraphics[width=10cm]{`r ap("momentum_train.png")`}
\end{figure}

The higher momentum, the faster SGD learns the weights on the training data, but if momentum is too large, the training and test error fluctuates.

## Momentum in practice

\begin{figure}
\centering
  \includegraphics[width=10cm]{`r ap("momentum_test.png")`}
\end{figure}

The higher momentum, the faster SGD learns the weights on the training data, but if momentum is too large, the training and test error fluctuates.
<!--

## Nestorov momentum

- Momentum aims to solve poor conditioning of the hessian but also variance
  in the stochastic gradient.

- Nesterov momentum modifies the algorithm such that the gradient is evaluated
  after the current velocity is applied:
  \begin{eqnarray*}
    \nu &\leftarrow& \varphi \nu - \alpha \nabla_\theta \Big[\frac{1}{m} \sum_{i} L(y^{(i)}, f(\tilde{x}^{(i)}, \theta + \varphi\nu) \Big] \\
    \theta &\leftarrow& \theta + \nu
  \end{eqnarray*}

- We can interpret Nesterov momentum as an attempt to add a correction factor
  to the basic method.

-->
