<<setup-child, include = FALSE>>=
library(knitr)
set_parent("../style/preamble.Rnw")
@

<<size = "scriptsize", include=FALSE>>=
source("code/functions.R")
@

\input{../latex-math/basic-math}
\input{../latex-math/basic-ml}
\input{../latex-math/ml-nn}

\lecturechapter{4}{Deep Learning- TrainingX}
\lecture{Deep Learning- TrainingX}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{Training Neural Networks}
\lz
Training of neural nets is composed of two iterative steps:
\lz
\begin{enumerate}
\item \textbf{Forward pass:} The information of the inputs flow through the model to produce a prediction. Based on that, we compute the empirical loss.
\lz
\item \textbf{Backward pass:} Information of the error of the predictions flows backwards through the model and weights are updated in such a way the error is reduced.
\end{enumerate}
\lz

The error is calculated by a loss function $L(y, f(x, \thetab))$ of the true target $y$ and the networks output $f(x, \thetab)$.
\framebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{itemize}
\item For regression, we typically use the L2 loss:
$$\Lxy = \frac{1}{2}(y - \fx)^2$$
\item For classification we typically apply binary/categorical cross entropy:
$$\Lxy = y \log \fx + (1 - y) \log(1 - \fx)$$
\item Evaluated on the data, we refer to it as the risk function:
$$\riske = \sumin \Lxyi,$$
\end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{Training Neural Networks}
\begin{itemize}
\item To minimize the risk, we need to exploit the method of gradient descent in numerical optimization.
\item At a point $\theta^{[t]}$ we can calculate the gradient $\nabla \risk$, which always points in the direction of the steepest ascent.
\item Thus, $-\nabla \risk$ points in the direction of the steepest descent!

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\item At a point $\theta^{[t]}$ during minimization, we can improve by doing the following step:
\[\thetab^{[t + 1 ]}  = \thetab^{[t]} - \alpha \nabla \risk\left(\thetab^{[t]}\right)\]
where $\alpha$ determines the length of the step and is called learning rate.
\end{itemize}
\framebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}
\scalebox{1}{\includegraphics{plots/hill.pdf}}
\end{figure}
\hspace{1cm} Example of gradient descent with $\theta=(w_1, w_2)$: \textit{"Walking down the hill, towards the valley."}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{Weight updates with Backpropagation}
\begin{itemize}
\item To update each weight $w \in \thetab$ in the network we need their gradients with regards to the risk.
\item Since weights are stacked in layers inside the network, we need to repeatedly apply the *chain rule of calculus*. This process is called \textbf{backpropagation}.
\item After obtaining the gradients we can update the weights by gradient descent
\[\thetab^{[t + 1]} = \thetab^{[t]} - \alpha \cdot \frac{1}{n} \cdot \sumin \nabla_\theta L\left(\yi, f(\xi ~|~ \thetab^{[t]})\right)\]
\end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{Weight updates with Backpropagation}
\begin{itemize}
\item Optimization algorithms that use the entire training set to compute updates in one huge step are called batch or deterministic. This is computationally very costly or often impossible.
\item Instead of letting the sum run over the whole dataset (batch mode) one can also let it run only over small subsets of size $m$ (minibatches).
\item With minibatches of size $m$, a full pass over the training set (called an epoch) consists of $\frac{n}{m}$ gradient updates.
\end{itemize}
\framebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \begin{algorithm}[H]
  \footnotesize
    \caption{Basic SGD pseudo code}
    \begin{algorithmic}[1]
    \State Initialize parameter vector $\thetab^{[0]}$

    \State $t \leftarrow 0$
    \While{stopping criterion not met}
    \State Randomly shuffle data and partition into minibatches $J_1, ..., J_K$ of size $m$
      \For{$k\in\{1,...,K\}$}
      \State $t \leftarrow t + 1$
      \State Compute gradient estimate with $J_k$:
      \[\hat{g}^{[t]} \leftarrow \frac{1}{m} \sum_{i \in J_k} \nabla_\theta L(\yi, f(\xi ~|~ \thetab^{[t-1]}))
      \]
      \State Apply update: $\thetab^{[t]} \leftarrow \thetab^{[t-1]} - \alpha \hat{g}^{[t]}$

      \EndFor


      \EndWhile
    \end{algorithmic}
  \end{algorithm}
  
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{Weight updates with Backpropagation}
\begin{itemize}
\item While SGD remains a popular optimization strategy, learning with it can sometimes be slow.
\item Momentum is designed to accelerate learning, especially when facing high curvature, small but consistent or noisy gradients:
\begin{eqnarray*}
\nu &\leftarrow& \varphi \nu - \alpha g(\theta) \\
\theta &\leftarrow& \theta + \nu
\end{eqnarray*}
\item Accumulate an exponentially decaying moving average of past gradients.
\end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{SGD with Momentum}
\begin{figure}
\scalebox{.8}{\includegraphics{plots/momentum.png}}
\end{figure}
The contour lines show a quadratic loss function with a poorly conditioned Hessian matrix. The to curves show how standard gradient descent (black) and momentum (red) learn when dealing with ravines.
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{Momentum in Practice}
\begin{figure}
\scalebox{.8}{\includegraphics{plots/momentum_train.png}}
\end{figure}
The higher momentum, the faster SGD learns the weights on the training data, but if momentum is too large, the training and test error fluctuates.
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{Early Stopping}
\begin{itemize}
\item When training with an iterative optimizer such as SGD, it is commonly the case that after a certain number of iterations, generalization error begins to increase even though training error continues to decrease.
\item \textbf{Early stopping} refers to stopping the algorithm early, before the generalization error increases.
\end{itemize}
\begin{figure}
\scalebox{.3}{\includegraphics{plots/earlystop.png}}
\end{figure}
After a certain number of iterations, the algorithm begins to overfit.
\framebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{How early stopping works:}
\begin{enumerate}
\item Split training data $(X^{(train)}, y^{(train)})$ into $(X^{(subtrain)}, y^{(subtrain)})$ and $(X^{(validation)}, y^{(validation)})$ (e.g. with a ratio of 2:1).
\item Use $(X^{(subtrain)}, y^{(subtrain)})$ and evaluate model using the $(X^{(validation)}, y^{(validation)})$.
\item Stop training when validation error stops decreasing (after a range of "patience" steps).
\item Use parameters of the previous step for the actual model.
\end{enumerate}
\framebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Strengths:}

\begin{itemize}
\item Effective and simple \& Periodical evaluation of validation error
\item Applicable to almost any model without adjustment
\item Combinable with other regularization methods
\end{itemize}

\textbf{Weaknesses:}

\begin{itemize}
\item Periodical evaluation of validation error
\item Less data for training $\rightarrow$ include $(X^{(validation)}, y^{(validation)})$ afterwards
\end{itemize}

\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{Further Regularization Strategies}
\textbf{Parameter penalties}
\begin{itemize}
\item Same as Rigde Regression/L2-Regularization
\item Often referred to as \textit{weight decay} since weights are pulled to zero if they are not updated by large enough values.
\end{itemize}
\begin{figure}
\scalebox{.4}{\includegraphics{plots/weight_decay.png}}
\end{figure}
\framebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Dropout}
\begin{itemize}
\item Force the network to generalize by reducing its capacity to memorize data.
\item Each neuron has a fixed probability to be deactivated at each training step.
\end{itemize}
\begin{figure}
\scalebox{.4}{\includegraphics{plots/subnet1.png}}
\end{figure}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\endlecture