---
title: 'Chapter 6: '
description:
  " This Chapter introduces bagging as method to increase the performance of trees. A modification of bagging leads to random forests. We explain the main idea of random forests, benchmark their performance with the methods seen so far and show how to quantify the impact of a single feature on the performance of the random forest as well as how to compute proximities between observations based on random forests."
type: chapter
prev: /chapter05
next: /chapter06
id: 1
---


<section class="c72e2d57">
  <h2 class="_5e0ebe7a">
  <a class="link-module-root-46224d00 link-module-hidden-7e2d93b5" href="/chapter06-01-forests-baggingensembles">Chapter 6.1: Bagging Ensembles</a>

  </h2>
  <p class="de526628">
  <a class="_46224d00 _7e2d93b5" href="/chapter06-01-forests-baggingensembles"> Bagging (bootstrap aggregation) is a method for combining many models into a meta-model, which often works much better than its individual components. In this Section, we present the basic idea of bagging and explain why and when bagging works.</a>
  </p>
</section>





<section class="c72e2d57">
  <h2 class="_5e0ebe7a">
  <a class="link-module-root-46224d00 link-module-hidden-7e2d93b5" href="/chapter06-02-forests-introduction">Chapter 6.2: Introduction</a>

  </h2>
  <p class="de526628">
  <a class="_46224d00 _7e2d93b5" href="/chapter06-02-forests-introduction"> In this Section we investigate random forests, a modification of bagging for trees. We illustrate the effect of the ensemble size and show how to compute out-of-bag error estimates.</a>
  </p>
</section>





<section class="c72e2d57">
  <h2 class="_5e0ebe7a">
  <a class="link-module-root-46224d00 link-module-hidden-7e2d93b5" href="/chapter06-03-forests-benchmarking">Chapter 6.3: Benchmarking Trees, Forests, and Bagging K-NN</a>

  </h2>
  <p class="de526628">
  <a class="_46224d00 _7e2d93b5" href="/chapter06-03-forests-benchmarking"> In this Section we compare the performance of random forests vs. (bagged) CART and (bagged) k-NN.</a>
  </p>
</section>





<section class="c72e2d57">
  <h2 class="_5e0ebe7a">
  <a class="link-module-root-46224d00 link-module-hidden-7e2d93b5" href="/chapter06-04-forests-featureimportance">Chapter 6.4: Feature Importance</a>

  </h2>
  <p class="de526628">
  <a class="_46224d00 _7e2d93b5" href="/chapter06-04-forests-featureimportance"> In a complex machine learning model, the contributions of the different features to the model performance are difficult to evaluate. The concept of feature importance allows to quantify this for random forests.</a>
  </p>
</section>





<section class="c72e2d57">
  <h2 class="_5e0ebe7a">
  <a class="link-module-root-46224d00 link-module-hidden-7e2d93b5" href="/chapter06-05-forests-proximitis">Chapter 6.5: Proximities</a>

  </h2>
  <p class="de526628">
  <a class="_46224d00 _7e2d93b5" href="/chapter06-05-forests-proximitis"> The term "proximity" means the "closeness" between pairs of cases. Proximities are calculated for each pair of observations and can be derived directly from random forests.</a>
  </p>
</section>





<section class="c72e2d57">
  <h2 class="_5e0ebe7a">
  <a class="link-module-root-46224d00 link-module-hidden-7e2d93b5" href="/chapter06-06-forests-discussion">Chapter 6.6: Discussion</a>

  </h2>
  <p class="de526628">
  <a class="_46224d00 _7e2d93b5" href="/chapter06-06-forests-discussion"> In this Section we discuss the advantages and disadvantages of random forests and explain that all advantages of trees also apply here.</a>
  </p>
</section>




