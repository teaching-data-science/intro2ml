---
title: 'Chapter 05: Classification and Regression Trees (CART)'
description:
  " This chapter introduces Classification And Regression Trees (CART), a well-established machine learning procedure. We explain the main idea and give details on splitting criteria, discuss computational aspects of growing a tree, and illustrate the idea of stopping criteria and pruning."
type: chapter
prev: /chapter04
next: /chapter06
id: 1
---


<section class="c72e2d57">
  <h2 class="_5e0ebe7a">
  <a class="_46224d00 _7e2d93b5" href="/chapter05-01-trees-introduction">Chapter 5.1: Introduction</a>

  </h2>
  <p class="de526628">
  <a class="_46224d00 _7e2d93b5" href="/chapter05-01-trees-introduction"> Decision trees are an important type of machine learning models and are of two main types: classification trees and regression trees. In this section, we explain the general idea of CART - Classification And Regression Trees.</a>
  </p>
</section>





<section class="c72e2d57">
  <h2 class="_5e0ebe7a">
  <a class="_46224d00 _7e2d93b5" href="/chapter05-02-trees-splittingcriteria">Chapter 5.2: Splitting Criteria</a>

  </h2>
  <p class="de526628">
  <a class="_46224d00 _7e2d93b5" href="/chapter05-02-trees-splittingcriteria"> CART algorithms require splitting criteria for trees, which are usually defined in terms of "impurity reduction". In this section we formalize the idea of splitting criteria and explain the details of splitting for both regression and classification.</a>
  </p>
</section>





<section class="c72e2d57">
  <h2 class="_5e0ebe7a">
  <a class="_46224d00 _7e2d93b5" href="/chapter05-03-trees-growingatree">Chapter 5.3: Growing a Tree</a>

  </h2>
  <p class="de526628">
  <a class="_46224d00 _7e2d93b5" href="/chapter05-03-trees-growingatree"> In this section, we explain how to grow a tree starting with an empty tree, a root node containing all the data. It will be shown that trees are grown by recursively applying greedy optimization to each node.</a>
  </p>
</section>





<section class="c72e2d57">
  <h2 class="_5e0ebe7a">
  <a class="_46224d00 _7e2d93b5" href="/chapter05-04-trees-computationalaspects">Chapter 5.4: Computational Aspects of Finding Splits</a>

  </h2>
  <p class="de526628">
  <a class="_46224d00 _7e2d93b5" href="/chapter05-04-trees-computationalaspects"> In this section, we explain the computational aspects of the node-splitting procedure, especially for nominal features. Additionally we illustrate how to deal with missing values.</a>
  </p>
</section>





<section class="c72e2d57">
  <h2 class="_5e0ebe7a">
  <a class="_46224d00 _7e2d93b5" href="/chapter05-05-trees-stoppingcriteria">Chapter 5.5: Stopping criteria & pruning</a>

  </h2>
  <p class="de526628">
  <a class="_46224d00 _7e2d93b5" href="/chapter05-05-trees-stoppingcriteria"> The recursive partitioning procedure used to grow a CART usually leads to problems such as exponential growth of computations, overfitting, and the horizon effect. To deal with these problems, we can use stopping criteria and pruning. In this section, we explain the basis of these two solutions.</a>
  </p>
</section>





<section class="c72e2d57">
  <h2 class="_5e0ebe7a">
  <a class="_46224d00 _7e2d93b5" href="/chapter05-06-trees-discussion">Chapter 5.6: Discussion</a>

  </h2>
  <p class="de526628">
  <a class="_46224d00 _7e2d93b5" href="/chapter05-06-trees-discussion"> In this section we discuss the advantages and disadvantages of CART and mention other tree methodologies.</a>
  </p>
</section>




