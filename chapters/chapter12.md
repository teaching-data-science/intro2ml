---
title: 'Chapter 12: Information Theory'
description:
  " This chapter covers basic information-theoretic concepts and discusses their relation to machine learning." 
type: chapter
prev: /chapter011
next: /chapter013
id: 8
---


<section class="c72e2d57">
  <h2 class="_5e0ebe7a">
  <a class="_46224d00 _7e2d93b5" href="/chapter12-01-info-entropy">Chapter 12.01: Entropy</a>

  </h2>
  <p class="de526628">
  <a class="_46224d00 _7e2d93b5" href="/chapter12-01-info-entropy"> In this section, we introduce the entropy as a concept which expresses the expected information for discrete random variables. </a>
  </p>
</section>





<section class="c72e2d57">
  <h2 class="_5e0ebe7a">
  <a class="_46224d00 _7e2d93b5" href="/chapter12-02-info-diffent">Chapter 12.02: Differential Entropy</a>

  </h2>
  <p class="de526628">
  <a class="_46224d00 _7e2d93b5" href="/chapter12-02-info-diffent"> In this section, we introduce the differential entropy as a concept which expresses the expected information for continuous random variables. </a>
  </p>
</section>





<section class="c72e2d57">
  <h2 class="_5e0ebe7a">
  <a class="_46224d00 _7e2d93b5" href="/chapter12-03-info-kl">Chapter 12.03: Kullback-Leibler Divergence</a>

  </h2>
  <p class="de526628">
  <a class="_46224d00 _7e2d93b5" href="/chapter12-03-info-kl"> In this section, we introduce the Kullback-Leibler divergence. </a>
  </p>
</section>





<section class="c72e2d57">
  <h2 class="_5e0ebe7a">
  <a class="_46224d00 _7e2d93b5" href="/chapter12-04-info-sourcecoding">Chapter 12.04: Entropy and Optimal Code Length</a>

  </h2>
  <p class="de526628">
  <a class="_46224d00 _7e2d93b5" href="/chapter12-04-info-sourcecoding"> In this section, we introduce source coding and discuss how entropy can be understood as optimal code length. </a>
  </p>
</section>





<section class="c72e2d57">
  <h2 class="_5e0ebe7a">
  <a class="_46224d00 _7e2d93b5" href="/chapter12-05-info-cross-entropy-kld">Chapter 12.05: Cross-Entropy, KL and Source Coding</a>

  </h2>
  <p class="de526628">
  <a class="_46224d00 _7e2d93b5" href="/chapter12-05-info-cross-entropy-kld"> In this section, we introduce the cross-entropy and discuss the connection between entropy, cross-entropy, and Kullback-Leibler-divergence. </a>
  </p>
</section>





<section class="c72e2d57">
  <h2 class="_5e0ebe7a">
  <a class="_46224d00 _7e2d93b5" href="/chapter12-06-info-ml">Chapter 12.06: Information Theory for Machine Learning</a>

  </h2>
  <p class="de526628">
  <a class="_46224d00 _7e2d93b5" href="/chapter12-06-info-ml"> In this section, we discuss how information-theoretic concepts are used in machine learning. </a>
  </p>
</section>





<section class="c72e2d57">
  <h2 class="_5e0ebe7a">
  <a class="_46224d00 _7e2d93b5" href="/chapter12-07-info-mutual-info">Chapter 12.07: Joint Entropy and Mutual Information</a>

  </h2>
  <p class="de526628">
  <a class="_46224d00 _7e2d93b5" href="/chapter12-07-info-mutual-info"> In this section, we introduce joint entropy, conditional entropy, and mutual information. </a>
  </p>
</section>




